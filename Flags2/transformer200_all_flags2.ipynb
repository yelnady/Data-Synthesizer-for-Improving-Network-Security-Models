{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5042be4",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c77946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "from tensorboard import default\n",
    "import torch.nn.functional as F\n",
    "from datetime import timedelta\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "sys.path.append('DG/gan')\n",
    "import gc\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7501f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43957f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 25396838400\n",
      "free     : 13020758016\n",
      "used     : 12376080384\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba6af9",
   "metadata": {},
   "source": [
    "# Features & Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f38ed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Features\n",
      "Feature: 1  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 2  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 3  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 4  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 5  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 6  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 7  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 8  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 9  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "\n",
      "Y Features\n",
      "Feature: 1  -- Normalization: None  -- gen_flag: False  -- Dim: 4\n"
     ]
    }
   ],
   "source": [
    "with open('data/google/data_feature_output.pkl', 'rb') as f:\n",
    "    data_feature = pickle.load(f)    \n",
    "with open('data/google/data_attribute_output.pkl', 'rb') as f:\n",
    "    data_attribute = pickle.load(f)\n",
    "\n",
    "    \n",
    "# data_feature is a list of 9 \"output.Output\" objects, where each object contains attrs -> (is_gen_flag, dim, normalization)\n",
    "print(\"X Features\")\n",
    "for i,feature in enumerate(data_feature):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)\n",
    "\n",
    "print(\"\\nY Features\")\n",
    "for i,feature in enumerate(data_attribute):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bdcd46",
   "metadata": {},
   "source": [
    "# Loading Real Train Data\n",
    "\n",
    "- Class0: 6250\n",
    "- Class1: 16124\n",
    "- Class2: 21273\n",
    "- Class3: 5278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4531c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all samples from this class that has two timesteps or more, class0 was 6529 data points, now it's 6250\n",
    "# 1- Get indices of current class_label\n",
    "# 2- Calculate lengths of the sequence samples\n",
    "# 3- Choose samples that are nonzero\n",
    "\n",
    "def get_one_class(X,Y,mask,class_label): # (X, Y, and mask) are the whole dataset that is consisted of many classes, Y is NOT One-Hot Encoded\n",
    "    indices_class_label = np.where(Y==class_label)\n",
    "    X,Y,mask = X[indices_class_label], Y[indices_class_label], mask[indices_class_label] \n",
    "    indices_non_zero = torch.nonzero(torch.sum(mask,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], mask[indices_non_zero]\n",
    "\n",
    "def get_n_samples(X,Y,mask,n_samples):\n",
    "    randomList = random.sample(range(0, Y.shape[0]), n_samples)\n",
    "    return X[randomList], Y[randomList], mask[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. --> So we do remove those ones by converting them to zeros, then remove from the list\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y,flag):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d75ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('data/google/data_train.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "\n",
    "real_train_X,real_train_Y_labels,real_train_flags = remove_zero_datapoints(real_train_X,real_train_Y_labels,real_train_flags)\n",
    "\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7bd8e",
   "metadata": {},
   "source": [
    "# The Tow Magic Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6193b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = real_train_flags.clone().unsqueeze(2)\n",
    "p2 = torch.zeros_like(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2846e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,length in enumerate(real_train_lengths):\n",
    "    p1[i,length-1] = 0\n",
    "    p2[i,length-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82460165",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_rows = torch.cat((p1,p2),2).float()\n",
    "real_train_X = torch.cat((real_train_X,torch.FloatTensor(magic_rows)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0551f53",
   "metadata": {},
   "source": [
    "# PyTorch Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c8faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649867b",
   "metadata": {},
   "source": [
    "S is the source sequence length, T is the target sequence length, B is the batch size, E is the feature number\n",
    "\n",
    "- src: (S, B, E)\n",
    "- src_mask: (S, S) # For Self-Attention\n",
    "- src_key_padding_mask: (B, S)   ---- The positions with the value of \"True\" will be ignored while the position with the value of False will be unchanged.\n",
    "- output: (T, B, E)\n",
    "\n",
    "In the paper n_hidden was 64 and d_model is 512, next we try n_hidden 2048, and d_model 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2377c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=11, d_model=512, n_heads=8, n_hidden=512, n_layers=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Time Series Transformer Model'\n",
    "        self.InputLinear = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_features) # The output of the encoder is similar to the input of the encoder, both are (B,S,d_model)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.activation1 = nn.Sigmoid()\n",
    "        self.activation2= nn.Softmax(dim=2)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e6)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask.to(device)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask,padding_mask):\n",
    "        src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask,padding_mask)\n",
    "        output = self.OutputLinear(output)\n",
    "        output1 = self.activation1(output[...,:(self.n_features-2)]) # output[...,:9] --> Actual 9 values\n",
    "        output2 = self.activation2(output[...,(self.n_features-2):])\n",
    "        return torch.cat((output1,output2),2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e59b5f",
   "metadata": {},
   "source": [
    "# Preparing Inputs, Masks, and Targets\n",
    "\n",
    "- Inputs, will be the original data, except the last actual timestep will be masked in the padding_mask\n",
    "- Targets, will be the original data, but shifted to the left one step, and added zero at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717e505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B = real_train_X.size(0)\n",
    "S = real_train_X.size(1)\n",
    "E = real_train_X.size(2)\n",
    "\n",
    "# 1- Shift the targets\n",
    "Input_shifted = real_train_X[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1,E))\n",
    "targets = torch.cat((Input_shifted,Zero_at_the_end),1)\n",
    "\n",
    "# 2- Shift the masks to be the same as targets\n",
    "\n",
    "real_train_masks = real_train_flags == 0 # True when padding, False when actual datapoint\n",
    "real_train_masks = real_train_masks[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1))==0\n",
    "real_train_masks = torch.cat((real_train_masks,Zero_at_the_end),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcbc2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################--------WINDOW SIZE-------------###########################################\n",
    "\n",
    "targets=  targets[:,:400]\n",
    "real_train_masks = real_train_masks[:,:400]\n",
    "real_train_X = real_train_X[:,:400]\n",
    "\n",
    "S = real_train_X.size(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427c7068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
       "         [9.2060e-03, 5.8230e-03, 1.0000e+00, 0.0000e+00],\n",
       "         [4.8378e-03, 6.0592e-02, 1.0000e+00, 0.0000e+00],\n",
       "         [2.3812e-03, 9.3676e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [2.4353e-03, 2.2530e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [4.4031e-03, 3.1798e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [7.3580e-03, 2.0960e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [2.8932e-04, 0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]),\n",
       " tensor([[9.2060e-03, 5.8230e-03, 1.0000e+00, 0.0000e+00],\n",
       "         [4.8378e-03, 6.0592e-02, 1.0000e+00, 0.0000e+00],\n",
       "         [2.3812e-03, 9.3676e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [2.4353e-03, 2.2530e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [4.4031e-03, 3.1798e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [7.3580e-03, 2.0960e-04, 1.0000e+00, 0.0000e+00],\n",
       "         [2.8932e-04, 0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]),\n",
       " tensor([False, False, False, False, False, False, False,  True,  True]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_train_X[4,:9,7:], targets[4,:9,7:], real_train_masks[4,:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9bf03",
   "metadata": {},
   "source": [
    "# Creating Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c684d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dataloader = {'shuffle': True,'num_workers':2,'batch_size':64} # No need to shuffle rn, they are all the same class\n",
    "dataset = torch.utils.data.TensorDataset(real_train_X, targets, real_train_masks)\n",
    "train_dataloader  = torch.utils.data.DataLoader(dataset, **params_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9b1f3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19325c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import sys\n",
    "\n",
    "# # These are the usual ipython objects, including this one you are creating\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a63a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters is  12635659\n"
     ]
    }
   ],
   "source": [
    "model = TimeSeriesTransformer().to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters is \",count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9fbc81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect(),torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc1a7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Epoch1-----------------------------\n",
      "torch.Size([64, 400])\n",
      "torch.Size([64, 400])\n",
      "torch.Size([64, 400])\n",
      "torch.Size([64, 400])\n",
      "torch.Size([64, 400])\n",
      "torch.Size([64, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f80ecdc3208>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/yelnady/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/rhome/yelnady/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b0922cdf76e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All Epochs Loss is\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_epochs_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-b0922cdf76e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, src_mask, n_epochs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TimeSeriesTransformer().to(device)\n",
    "\n",
    "# We need to swap the axis since Transformer takes (S, B, E), we do that using permute(1,0,2)\n",
    "\n",
    "src_mask = model.generate_square_subsequent_mask(S).to(device)\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "def train(model,train_dataloader,src_mask,n_epochs):\n",
    "    time_all = time()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  \n",
    "    losses = []\n",
    "    all_epochs_loss = []\n",
    "    model.train()\n",
    "    MSE_loss = nn.MSELoss(reduction='none')\n",
    "    BCE_loss = torch.nn.BCELoss(reduction='none')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print('--------------------------Epoch{}-----------------------------'.format(epoch+1))\n",
    "        time0 = time()\n",
    "        one_epoch_loss = []\n",
    "        gc.collect(),torch.cuda.empty_cache()\n",
    "        for idx,(X,target,padding_mask) in enumerate(train_dataloader):\n",
    "            X = X.permute(1,0,2) # [S,B,E]\n",
    "            optimizer.zero_grad()\n",
    "            Y_predicted = model(X.to(device),src_mask,padding_mask.to(device))\n",
    "            print(padding_mask.shape)\n",
    "            Y_predicted = Y_predicted.permute(1,0,2)\n",
    "            \n",
    "            #--------------------------------------------LOSS MSE---------------------------------------------------#\n",
    "            gc.collect(),torch.cuda.empty_cache()\n",
    "            mse_loss = MSE_loss(Y_predicted, target.to(device))\n",
    "#             bce_loss = BCE_loss(Y_predicted[...,-2:], target[...,-2:].to(device))\n",
    "#             print(Y_predicted[...,-2:][0,:10])\n",
    "#             print(target[...,-2:][0,:10])\n",
    "#             print(padding_mask[0])\n",
    "#             print()\n",
    "\n",
    "\n",
    "#             loss = torch.cat((mse_loss,bce_loss),2)\n",
    "            loss = mse_loss\n",
    "\n",
    "            # 1- Use reduction='none' loss, and calculate MSE and BCE for the first 9 features and flags respectively\n",
    "            # 2- Multiply (element-wise) the loss by the flags (~padding_mask) to cancel any unwanted values\n",
    "            # 3- Sum loss across the features\n",
    "            # 4- Transpose loss, and bmm(loss,padding_mask) -> (B,1,1)\n",
    "            # 5- Calculate mean or sum of the batch losses\n",
    "            \n",
    "            flags = (~padding_mask).unsqueeze(2).float().to(device) # [B,S,1]\n",
    "            lengths = torch.sum(flags,1).long()\n",
    "            loss = torch.mul(loss,flags)\n",
    "            loss = torch.mean(loss,2) #Sum across the features\n",
    "            loss = torch.sum(loss,1) #Sum across the timesteps\n",
    "            loss = loss / lengths #get the mean loss of each sample (divide each sample by its length)\n",
    "            loss = torch.mean(loss)\n",
    "            loss.backward()            \n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            one_epoch_loss.append(loss.item())\n",
    "        \n",
    "           \n",
    "            #------------------------------------------END LOSS-----------------------------------------------------#\n",
    "            del X\n",
    "            del target\n",
    "            \n",
    "            if ((idx+1)%50==0):\n",
    "                print(\"Batch {}/{}\".format(idx+1,len(train_dataloader)))\n",
    "\n",
    "        print(\"Epoch {} Loss is {}\".format(epoch+1,np.mean(one_epoch_loss)))\n",
    "        print(\"Epoch {} - Time (in minutes) is {}\".format(epoch+1,timedelta(seconds=(time()-time0))))\n",
    "        all_epochs_loss.append(np.mean(one_epoch_loss))\n",
    "    \n",
    "    print(\"Total Time (in minutes) is {}\".format( timedelta(seconds=(time()-time_all))))\n",
    "    print(\"All Epochs Loss is\\n\",all_epochs_loss)\n",
    "    \n",
    "train(model,train_dataloader,src_mask,n_epochs=400)\n",
    "\n",
    "[[False,F,F,F,T,T,T]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'class_all_weights_flags2_bce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time (in minutes) is 0:07:51.223232\n",
    "# Epoch 1 Loss is 2.5318157373690138 then 2.1970 then 2.036 then 1.9373"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
