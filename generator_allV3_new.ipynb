{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd804dce",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fde077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "import seaborn as sns\n",
    "from tensorboard import default\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "sys.path.append('DG/gan')\n",
    "import gc\n",
    "print(device)\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fe389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 50962169856\n",
      "free     : 50958106624\n",
      "used     : 4063232\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafd34f",
   "metadata": {},
   "source": [
    "# Import Real Training Data to Generate New Data from it.\n",
    "\n",
    "### Actual Distribution\n",
    "- Class0: 6250\n",
    "- Class1: 16124\n",
    "- Class2: 21273\n",
    "- Class3: 5278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9cee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_class(X,Y,flag,class_label): # (X, Y, and flag) are the whole dataset that is consisted of many classes, Y is NOT One-Hot Encoded\n",
    "    indices_class_label = np.where(Y==class_label)\n",
    "    X,Y,flag = X[indices_class_label], Y[indices_class_label], flag[indices_class_label] \n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]\n",
    "\n",
    "def get_n_samples(X,Y,flag,n_samples):\n",
    "    randomList = random.sample(range(0, Y.shape[0]), n_samples)\n",
    "    return X[randomList], Y[randomList], flag[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. --> So we do remove those ones by converting them to zeros, then remove from the list\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y,flag):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98039037",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('data/google/data_train_reduced.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "#------------------------------------------------------------------Loading One Class------------------------------------------------\n",
    "real_train_X, real_train_Y_labels, real_train_flags= remove_zero_datapoints(real_train_X, real_train_Y_labels, real_train_flags)\n",
    "\n",
    "# The pading mask need to be inverted \n",
    "\n",
    "padding_mask = real_train_flags == 0 # True when padding, False when considering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee02bed",
   "metadata": {},
   "source": [
    "# PyTorch Transformer Model\n",
    "\n",
    "- Later, we need to remove this from here and put in a separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c89ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe) # ,persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "    \n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=9, d_model=256, n_heads=8, n_hidden=256, n_layers=8, dropout=0.0,S=400):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Time Series Transformer Model'\n",
    "        self.InputLinear = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding2(d_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout, batch_first = True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_features) # The output of the encoder is similar to the input of the encoder, both are (B,S,d_model)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e7)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask,padding_mask):\n",
    "        src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask,padding_mask)\n",
    "        output = self.OutputLinear(output)\n",
    "        output = self.activation(output) # output[...,:9] --> Actual 9 values\n",
    "        return output\n",
    "    \n",
    "    \n",
    "model = TimeSeriesTransformer().to(device)\n",
    "\n",
    "# # ck = torch.load('lightning_logs/version_9/checkpoints/epoch=49-step=9599.ckpt')['state_dict']\n",
    "model.load_state_dict(torch.load(\"W_transformer_allV3_new\"))\n",
    "\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d0c",
   "metadata": {},
   "source": [
    "# Generating New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91cd32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# The following is the generating part #################################\n",
    "\n",
    "# Returns: X (The data)\n",
    "# Returns: masks (e.g. [False,Flase,True,True,True,....,True]), False is the actual Data\n",
    "\n",
    "resulted_masks = []\n",
    "generated_dataset_Y=[]\n",
    "generated_dataset_X=[]\n",
    "\n",
    "def generate_dataset(X,Y,masks,n_seed,n_samples,max_length):\n",
    "    for n in range(n_samples):\n",
    "        \n",
    "        datapoint,y,mask = get_n_samples(X,Y,masks,n_samples=1) # The first 10 timesteps of just one sample\n",
    "        datapoint = datapoint[:,:n_seed] \n",
    "        datapoint_len = torch.sum(~mask) #Flip and count, you will get the actual length to generate likewise\n",
    "        mask = mask[:,:n_seed] \n",
    "        E = datapoint.size(2)\n",
    "        S = datapoint.size(1)\n",
    "        for t in range(max_length-n_seed): # Loop until 400 timesteps\n",
    "            src_mask = model.generate_square_subsequent_mask(S)\n",
    "\n",
    "            predicted = model(datapoint.to(device),src_mask.to(device),mask.to(device)).cpu() # [S,B,E] --> We want just the predicted timestep S\n",
    "            one_new_timestep=predicted[:,-1].unsqueeze(1)\n",
    "            datapoint = torch.cat((datapoint,one_new_timestep),1) # add the forecasted timestep\n",
    "            mask = torch.cat((mask,torch.tensor([[False]])),1 )\n",
    "            S = datapoint.size(1)\n",
    "    \n",
    "            if S == datapoint_len :\n",
    "                datapoint = torch.cat((datapoint,torch.zeros((1,max_length-S,E))), 1).cpu()# Pad remainings with zero\n",
    "                mask =  torch.cat((mask,torch.full((1,max_length-S),True)),1)\n",
    "        \n",
    "                break\n",
    "        resulted_masks.append(mask.numpy())\n",
    "        generated_dataset_X.append(datapoint.squeeze().detach().numpy())\n",
    "        generated_dataset_Y.append(y.item())\n",
    "        del mask\n",
    "        del datapoint\n",
    "        del one_new_timestep\n",
    "        gc.collect(),torch.cuda.empty_cache()\n",
    "        \n",
    "        if (n%100==0):\n",
    "            print('{}/{}'.format(n,n_samples))\n",
    "        if ((n+1)%1000==0):\n",
    "             np.savez('npz_transformer_allV3_new',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n",
    "\n",
    "max_length = 400\n",
    "n_seed = 2\n",
    "# Padding Mask Fed here is the Mask where \"False is Real Data\", True is masked and ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a648f5c-83e8-4843-8530-e257e5144aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c2da05b22d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-88956920957f>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[0;34m(X, Y, masks, n_seed, n_samples, max_length)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [S,B,E] --> We want just the predicted timestep S\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mone_new_timestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdatapoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_new_timestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add the forecasted timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-16831cca8a61>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, padding_mask)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output[...,:9] --> Actual 9 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTransformer\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \"\"\"\n\u001b[0;32m--> 320\u001b[0;31m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0m\u001b[1;32m    321\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[1;32m    322\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1032\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   5082\u001b[0m     \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5083\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5084\u001b[0;31m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5086\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "real_train_X0 ,real_train_Y_labels0 ,padding_mask0= get_one_class(real_train_X ,real_train_Y_labels ,padding_mask,0)\n",
    "real_train_X1 ,real_train_Y_labels1 ,padding_mask1= get_one_class(real_train_X ,real_train_Y_labels ,padding_mask,1)\n",
    "real_train_X2 ,real_train_Y_labels2 ,padding_mask2= get_one_class(real_train_X ,real_train_Y_labels ,padding_mask,2)\n",
    "real_train_X3 ,real_train_Y_labels3 ,padding_mask3= get_one_class(real_train_X ,real_train_Y_labels ,padding_mask,3)\n",
    "\n",
    "\n",
    "generate_dataset(real_train_X0 ,real_train_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_train_X0.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X1 ,real_train_Y_labels1 ,padding_mask1,n_seed=n_seed,n_samples=real_train_X1.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X2 ,real_train_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_train_X2.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X3 ,real_train_Y_labels3 ,padding_mask3,n_seed=n_seed,n_samples=real_train_X3.size(0),max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193017c-1930-4e3e-bdaa-29a9fcb36697",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('npz_transformer_allV3_new',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
