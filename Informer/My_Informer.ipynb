{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "from tensorboard import default\n",
    "import torch.nn.functional as F\n",
    "from datetime import timedelta\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "sys.path.append('../DG/gan')\n",
    "import gc\n",
    "print(device)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.encoder import Encoder, EncoderLayer, ConvLayer, EncoderStack\n",
    "from models.attn import FullAttention, ProbAttention, AttentionLayer\n",
    "from models.embed import TokenEmbedding, PositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu111'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 50962169856\n",
      "free     : 41514631168\n",
      "used     : 9447538688\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features & Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Features\n",
      "Feature: 1  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 2  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 3  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 4  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 5  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 6  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 7  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 8  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 9  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "\n",
      "Y Features\n",
      "Feature: 1  -- Normalization: None  -- gen_flag: False  -- Dim: 4\n"
     ]
    }
   ],
   "source": [
    "with open('../data/google/data_feature_output.pkl', 'rb') as f:\n",
    "    data_feature = pickle.load(f)    \n",
    "with open('../data/google/data_attribute_output.pkl', 'rb') as f:\n",
    "    data_attribute = pickle.load(f)\n",
    "\n",
    "    \n",
    "# data_feature is a list of 9 \"output.Output\" objects, where each object contains attrs -> (is_gen_flag, dim, normalization)\n",
    "print(\"X Features\")\n",
    "for i,feature in enumerate(data_feature):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)\n",
    "\n",
    "print(\"\\nY Features\")\n",
    "for i,feature in enumerate(data_attribute):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Real Train Data\n",
    "\n",
    "- Class0: 6250\n",
    "- Class1: 16124\n",
    "- Class2: 21273\n",
    "- Class3: 5278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all samples from this class that has two timesteps or more, class0 was 6529 data points, now it's 6250\n",
    "# 1- Get indices of current class_label\n",
    "# 2- Calculate lengths of the sequence samples\n",
    "# 3- Choose samples that are nonzero\n",
    "\n",
    "def get_one_class(X,Y,mask,class_label): # (X, Y, and mask) are the whole dataset that is consisted of many classes, Y is NOT One-Hot Encoded\n",
    "    indices_class_label = np.where(Y==class_label)\n",
    "    X,Y,mask = X[indices_class_label], Y[indices_class_label], mask[indices_class_label] \n",
    "    indices_non_zero = torch.nonzero(torch.sum(mask,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], mask[indices_non_zero]\n",
    "\n",
    "def get_n_samples(X,Y,mask,n_samples):\n",
    "    randomList = random.sample(range(0, Y.shape[0]), n_samples)\n",
    "    return X[randomList], Y[randomList], mask[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. --> So we do remove those ones by converting them to zeros, then remove from the list\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y,mask):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(mask,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], mask[indices_non_zero]\n",
    "\n",
    "\n",
    "def prepare_attn_mask(padding_mask):\n",
    "    attn_mask = torch.zeros((padding_mask.shape[0],padding_mask.shape[1],padding_mask.shape[1]), dtype=torch.bool) #[B,L,L]\n",
    "    for idx in range(len(padding_mask)):\n",
    "        x = attn_mask.shape[1]\n",
    "        attn_mask[idx] = (~torch.logical_and(~padding_mask[idx].view(-1,1).expand(x,x),\n",
    "                                              ~padding_mask[idx].expand(x,x))).logical_or(torch.ones(x,x).triu(1))\n",
    "    return attn_mask #[B,L,L]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('../data/google/data_train.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "\n",
    "real_train_X,real_train_Y_labels,real_train_flags = remove_zero_datapoints(real_train_X,real_train_Y_labels,real_train_flags)\n",
    "\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Inputs, Masks, and Targets\n",
    "\n",
    "- Inputs, will be the original data, except the last actual timestep will be masked in the padding_mask\n",
    "- Targets, will be the original data, but shifted to the left one step, and added zero at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = real_train_X.size(0)\n",
    "S = real_train_X.size(1)\n",
    "E = real_train_X.size(2)\n",
    "\n",
    "# 1- Shift the targets\n",
    "Input_shifted = real_train_X[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1,E))\n",
    "targets = torch.cat((Input_shifted,Zero_at_the_end),1)\n",
    "\n",
    "# 2- Shift the masks to be the same as targets\n",
    "\n",
    "real_train_masks = real_train_flags == 0 # True when padding, False when actual datapoint\n",
    "real_train_masks = real_train_masks[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1))==0\n",
    "real_train_masks = torch.cat((real_train_masks,Zero_at_the_end),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################--------WINDOW SIZE-------------###########################################\n",
    "\n",
    "window_size = S = 400\n",
    "targets=  targets[:,:window_size]\n",
    "real_train_masks = real_train_masks[:,:window_size]\n",
    "real_train_X = real_train_X[:,:window_size]\n",
    "\n",
    "# S = real_train_X.size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64\n",
    "params_dataloader = {'shuffle': True,'num_workers':2,'batch_size':B} # No need to shuffle rn, they are all the same class\n",
    "dataset = torch.utils.data.TensorDataset(real_train_X, targets, real_train_masks)\n",
    "train_dataloader  = torch.utils.data.DataLoader(dataset, **params_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 50962169856\n",
      "free     : 46757511168\n",
      "used     : 4204658688\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informer: DataEmbedding(x_enc, x_mark_enc):enc_out  --> encoder(enc_out,enc_self_mask):enc_out, attns \n",
    "#           DataEmbedding(x_dec, x_mark_dec):dec_out  --> decoder(dec_out, enc_out, dec_self_mask, dec_enc_mask):dec_out\n",
    "#           projection(dec_out): dec_out [Batch, n_timesteps , n_features]\n",
    "\n",
    "# Encoder takes 3 params (list of attn_layers/Encoder_Layer, List of Conv Layer if distil, normalization layer/nn.LayerNorm) -> returns x, attn\n",
    "# EncoderLayer takes 4 params (AttentionLayer, d_model, n_hidden = n_hidden or 4*d_model, dropout=0.1, activation=\"relu\") -> returns x, attns\n",
    "# Attn takes 5 params (mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False)\n",
    "\n",
    "\n",
    "\n",
    "#  d_model=512; n_heads=8; e_layers=3; d_ff=512; n_features = 9; dropout=0.0; attn='full'; activation='gelu'; ; distil=False; mix=True;\n",
    "        \n",
    "# We need to change the gelu and relu to allow for Sigmoid and Tanh in the Encoder\n",
    "# output_attention = Flase because Encoder returns  (enc_out, attns)\n",
    "# d_ff is like d_hidden\n",
    "\n",
    "\n",
    "# Before Encoder we need to feed a positional encoding and linear layer to map the dimensions to that encoder\n",
    "# After Encoder we need to remap the dimensiosn \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=9, d_model=512, n_heads=8, n_hidden=512, e_layers=3, dropout=0.0,seq_length = window_size,\n",
    "                 attn='prob', mask_flag = True, factor=5,activation='gelu',distil=False, mix=False, output_attention = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in=n_features, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.InputLinear = nn.Linear(n_features, d_model)\n",
    "        self.attn = attn\n",
    "        self.d_model = d_model\n",
    "        Attn = ProbAttention if attn=='prob' else FullAttention\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(Attn(mask_flag=mask_flag, factor=factor, attention_dropout=dropout, output_attention=output_attention), \n",
    "                                d_model, n_heads, mix),\n",
    "                    d_model,\n",
    "                    n_hidden,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            [ConvLayer(d_model) for l in range(e_layers-1)] if distil else None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "        #-----------------------------------------------------------------------------------------------------------------------------#\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_features)\n",
    "        self.init_weights()\n",
    "        self.activation= nn.Sigmoid()\n",
    "        self.end_conv1 = nn.Conv1d(in_channels=102, out_channels=seq_length, kernel_size=1, bias=True)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src, attn_mask): #attn_mask is one mask that provides everything for us\n",
    "#         src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.value_embedding(src) + self.position_embedding(src) # + self.temporal_embedding(x_mark)\n",
    "        src = self.dropout(src)\n",
    "        \n",
    "        output,attns = self.encoder(src,attn_mask) #attn_mask is being passed to FullAttention or ProbAttention\n",
    "\n",
    "#         output = self.end_conv1(output)\n",
    "        \n",
    "        output = self.OutputLinear(output)\n",
    "        output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect(),torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,target,mask_sample =iter(train_dataloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Epoch1-----------------------------\n",
      "tensor(19.8292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(2.3205, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f45ebe9b160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/yelnady/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/rhome/yelnady/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8ceafc938120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All Epochs Loss is\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_epochs_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-8ceafc938120>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_attn_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mY_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2112cff8d287>\u001b[0m in \u001b[0;36mprepare_attn_mask\u001b[0;34m(padding_mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         attn_mask[idx] = (~torch.logical_and(~padding_mask[idx].view(-1,1).expand(x,x),\n\u001b[0m\u001b[1;32m     28\u001b[0m                                               ~padding_mask[idx].expand(x,x))).logical_or(torch.ones(x,x).triu(1))\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;31m#[B,L,L]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MyInformer().to(device)\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "def train(model,train_dataloader,n_epochs):\n",
    "    time_all = time()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  \n",
    "    losses = []\n",
    "    all_epochs_loss = []\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('--------------------------Epoch{}-----------------------------'.format(epoch+1))\n",
    "        time0 = time()\n",
    "        one_epoch_loss = []\n",
    "        for idx,(X,target,padding_mask) in enumerate(train_dataloader):\n",
    "\n",
    "            optimizer.zero_grad(),gc.collect(),torch.cuda.empty_cache()\n",
    "\n",
    "            attn_mask = prepare_attn_mask(padding_mask)\n",
    "            \n",
    "            Y_predicted = model(X.to(device),attn_mask)\n",
    "\n",
    "            #--------------------------------------------LOSS MSE---------------------------------------------------#\n",
    "            mse_loss = nn.MSELoss(reduction='none')\n",
    "            loss = mse_loss(Y_predicted, target.to(device))\n",
    "            \n",
    "            # 1- Use reduction='none' loss, and calculate MSE for the first 9 features only\n",
    "            # 2- Sum the loss across features -> (B,S)\n",
    "            # 3- Unsqueeze to use bmm -> loss: (B,S,1) , ~padding_mask.float(): (B,S,1)\n",
    "            # 4- Transpose loss, and bmm(loss,padding_mask) -> (B,1,1)\n",
    "            # 5- Calculate mean or sum of the batch losses\n",
    "            loss = torch.sum(loss,2)\n",
    "            padding_mask = (~padding_mask).unsqueeze(2).float().to(device)\n",
    "            loss = loss.unsqueeze(2).permute(0,2,1)\n",
    "            \n",
    "            loss = torch.bmm(loss,padding_mask).mean()\n",
    "            print(loss)\n",
    "            #-------------------------------------LOSS Cross Entropy-----------------------------------------------#\n",
    "            loss.backward()            \n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            one_epoch_loss.append(loss.item())\n",
    "        \n",
    "           \n",
    "            #------------------------------------------END LOSS-----------------------------------------------------#\n",
    "            del X\n",
    "            del target\n",
    "            \n",
    "            if ((idx+1)%50==0):\n",
    "                print(\"Batch {}/{}\".format(idx+1,len(train_dataloader)))\n",
    "\n",
    "        print(\"Epoch {} Loss is {}\".format(epoch+1,np.mean(one_epoch_loss)))\n",
    "        print(\"Epoch {} - Time (in minutes) is {}\".format(epoch+1,timedelta(seconds=(time()-time0))))\n",
    "        all_epochs_loss.append(np.mean(one_epoch_loss))\n",
    "    if ((epoch+1)%100==0):\n",
    "            torch.save(model.state_dict(), 'class_all_weights_informer')\n",
    "       \n",
    "    print(\"Total Time (in minutes) is {}\".format( timedelta(seconds=(time()-time_all))))\n",
    "    print(\"All Epochs Loss is\\n\",all_epochs_loss)\n",
    "    \n",
    "train(model,train_dataloader,n_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob gives 0.282603288663466 in 0:08:23 minutes \n",
    "# full gives 0.13638 in 0:10:14\n",
    "\n",
    "# Full gives 0.1431730 using just n_layers = 3 in just 0:07:09 minutes\n",
    "#prob gives 0.1298 using  just n_layers = 3 in just 0:07:11.3\n",
    "\n",
    "\n",
    "# Using distill and 3 layers only I got -> 1.43092 then 0.078535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'class_all_weights_informer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_flag doesn't provide anything related to our paddings, but we can provide our own matrix\n",
    "# The weight mask, which is the combination of the padding and causal masks, is used to know which positions to fill with \n",
    "# −∞ before computing the softmax, which will be zero after it.\n",
    "\n",
    "# first src_mask is used to block specific positions from attending and then key_padding_mask is used to block attending to pad tokens.\n",
    "\n",
    "\n",
    "# In ProbAttention or FullAttention, if mask_flag=True and attn_mask {given in encoder forward} = None --> Informer makes Attention Matrix Mask for you\n",
    "#                                  , if mask_flag=True and given mask --> Informer just converts True values to (-np.inf)\n",
    "#                                  , if mask_flag=False doesn't care about any type of masks at all\n",
    "\n",
    "#IMPORTANT -> We need just to send a attn_mask where False is Important, and True to ignore and it will be set to (-np.inf) by mask_flag=True\n",
    "\n",
    "# torch.masked_fill_(matrix,value): Fills elements of tensor with value where mask is True. \n",
    "\n",
    "# Mask Shape -> mask_shape = [B, 1, L, L] -> [B, n_heads, L, L]\n",
    "\n",
    "# The mask we provide need to have an attribute called mask as done in utils/masking so I build class MyProbMask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will pass everything ready in the attn_mask from EncoderLayer.forward() -> AttentionLayer.forward() -> ProbAttention.forward(attn_mask) -> _update_context(attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output_seq_len=label_len+pred_len because in decoder we have label_len is the history provided to be the X_token as in Figure1, then we tell it to predict the following lengths, with a total of output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to see difference between PropAttention and FullAttention\n",
    "# We need to know the difference implementations for attn_matrix in Prob and Full (Currently using Full)\n",
    "# We need to see why distill = False, makes the dimensions correct and ignores the conv layers\n",
    "\n",
    "# We need to understand factor and mix params\n",
    "# We need to know differnce between Encoder  and EncoderStack\n",
    "\n",
    "# Current Implementation -> No Linear Layer at all! and uses Full Attention which causes problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
