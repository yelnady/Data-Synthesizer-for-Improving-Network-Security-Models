{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hearing-operation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "from tensorboard import default\n",
    "import torch.nn.functional as F\n",
    "from datetime import timedelta\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "sys.path.append('../DG/gan')\n",
    "import gc\n",
    "print(device)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.masking import TriangularCausalMask, ProbMask\n",
    "from models.encoder import Encoder, EncoderLayer, ConvLayer, EncoderStack\n",
    "from models.decoder import Decoder, DecoderLayer\n",
    "from models.attn import FullAttention, ProbAttention, AttentionLayer\n",
    "from models.embed import TokenEmbedding, PositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pregnant-compilation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu111'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "revolutionary-financing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 50962169856\n",
      "free     : 50958106624\n",
      "used     : 4063232\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-constitution",
   "metadata": {},
   "source": [
    "# Features & Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outer-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Features\n",
      "Feature: 1  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 2  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 3  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 4  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 5  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 6  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 7  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 8  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "Feature: 9  -- Normalization: Normalization.ZERO_ONE  -- gen_flag: False  -- Dim: 1\n",
      "\n",
      "Y Features\n",
      "Feature: 1  -- Normalization: None  -- gen_flag: False  -- Dim: 4\n"
     ]
    }
   ],
   "source": [
    "with open('../data/google/data_feature_output.pkl', 'rb') as f:\n",
    "    data_feature = pickle.load(f)    \n",
    "with open('../data/google/data_attribute_output.pkl', 'rb') as f:\n",
    "    data_attribute = pickle.load(f)\n",
    "\n",
    "    \n",
    "# data_feature is a list of 9 \"output.Output\" objects, where each object contains attrs -> (is_gen_flag, dim, normalization)\n",
    "print(\"X Features\")\n",
    "for i,feature in enumerate(data_feature):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)\n",
    "\n",
    "print(\"\\nY Features\")\n",
    "for i,feature in enumerate(data_attribute):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-front",
   "metadata": {},
   "source": [
    "# Loading Real Train Data\n",
    "\n",
    "- Class0: 6250\n",
    "- Class1: 16124\n",
    "- Class2: 21273\n",
    "- Class3: 5278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "waiting-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all samples from this class that has two timesteps or more, class0 was 6529 data points, now it's 6250\n",
    "# 1- Get indices of current class_label\n",
    "# 2- Calculate lengths of the sequence samples\n",
    "# 3- Choose samples that are nonzero\n",
    "\n",
    "def get_one_class(X,Y,mask,class_label): # (X, Y, and mask) are the whole dataset that is consisted of many classes, Y is NOT One-Hot Encoded\n",
    "    indices_class_label = np.where(Y==class_label)\n",
    "    X,Y,mask = X[indices_class_label], Y[indices_class_label], mask[indices_class_label] \n",
    "    indices_non_zero = torch.nonzero(torch.sum(mask,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], mask[indices_non_zero]\n",
    "\n",
    "def get_n_samples(X,Y,mask,n_samples):\n",
    "    randomList = random.sample(range(0, Y.shape[0]), n_samples)\n",
    "    return X[randomList], Y[randomList], mask[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. --> So we do remove those ones by converting them to zeros, then remove from the list\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y,mask):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(mask,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], mask[indices_non_zero]\n",
    "\n",
    "\n",
    "def prepare_attn_mask(padding_mask):\n",
    "    attn_mask = torch.zeros((padding_mask.shape[0],padding_mask.shape[1],padding_mask.shape[1]), dtype=torch.bool) #[B,L,L]\n",
    "    for idx in range(len(padding_mask)):\n",
    "        x = attn_mask.shape[1]\n",
    "        attn_mask[idx] = (~torch.logical_and(~padding_mask[idx].view(-1,1).expand(x,x),\n",
    "                                              ~padding_mask[idx].expand(x,x))).logical_or(torch.ones(x,x).triu(1))\n",
    "    return attn_mask #[B,L,L]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "important-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('../data/google/data_train.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "\n",
    "real_train_X,real_train_Y_labels,real_train_flags = remove_zero_datapoints(real_train_X,real_train_Y_labels,real_train_flags)\n",
    "\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()\n",
    "real_train_masks = real_train_flags == 0 # True when padding, False when actual datapoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-subscription",
   "metadata": {},
   "source": [
    "# The Magic Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lyric-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "graduate-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_rows = []\n",
    "for n_length in real_train_lengths:\n",
    "    last_number = 1\n",
    "    n_length=min(max_length,n_length.item())\n",
    "    step = (1-0.5)/n_length\n",
    "    magic_row = []\n",
    "    # Fill with magic numbers\n",
    "    for _ in range(n_length-1):\n",
    "        last_number -=step\n",
    "        magic_row.append(last_number)\n",
    "    # Fill with zeros   \n",
    "    magic_row.extend([0]*(max_length - (n_length-1)))\n",
    "    magic_rows.append(magic_row)\n",
    "magic_rows = np.array(magic_rows)\n",
    "magic_rows=np.expand_dims(magic_rows,2)\n",
    "np.savez('magic_rows',magic=magic_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "canadian-consistency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48925, 2500, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magic_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "secure-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_X = torch.cat((real_train_X,torch.FloatTensor(magic_rows)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-faith",
   "metadata": {},
   "source": [
    "# Preparing Inputs, Masks, and Targets\n",
    "\n",
    "- Inputs, will be the original data, except the last actual timestep will be masked in the padding_mask\n",
    "- Targets, will be the original data, but shifted to the left one step, and added zero at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jewish-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = real_train_X.size(0)\n",
    "S = real_train_X.size(1)\n",
    "E = real_train_X.size(2)\n",
    "\n",
    "# 1- Shift the targets\n",
    "Input_shifted = real_train_X[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1,E))\n",
    "targets = torch.cat((Input_shifted,Zero_at_the_end),1)\n",
    "\n",
    "# 2- Shift the masks to be the same as targets\n",
    "\n",
    "real_train_masks = real_train_masks[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1))==0\n",
    "real_train_masks = torch.cat((real_train_masks,Zero_at_the_end),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ahead-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################--------WINDOW SIZE-------------###########################################\n",
    "\n",
    "window_size = S = 400\n",
    "targets=  targets[:,:window_size]\n",
    "real_train_masks = real_train_masks[:,:window_size]\n",
    "real_train_X = real_train_X[:,:window_size]\n",
    "\n",
    "# S = real_train_X.size(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-hotel",
   "metadata": {},
   "source": [
    "# Creating Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nasty-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 100\n",
    "params_dataloader = {'shuffle': True,'num_workers':2,'batch_size':B} # No need to shuffle rn, they are all the same class\n",
    "dataset = torch.utils.data.TensorDataset(real_train_X, targets, real_train_masks)\n",
    "train_dataloader  = torch.utils.data.DataLoader(dataset, **params_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "talented-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 50962169856\n",
      "free     : 50958106624\n",
      "used     : 4063232\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-collar",
   "metadata": {},
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nominated-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informer: DataEmbedding(x_enc, x_mark_enc):enc_out  --> encoder(enc_out,enc_self_mask):enc_out, attns \n",
    "#           DataEmbedding(x_dec, x_mark_dec):dec_out  --> decoder(dec_out, enc_out, dec_self_mask, dec_enc_mask):dec_out\n",
    "#           projection(dec_out): dec_out [Batch, n_timesteps , n_features]\n",
    "\n",
    "# Encoder takes 3 params (list of attn_layers/Encoder_Layer, List of Conv Layer if distil, normalization layer/nn.LayerNorm) -> returns x, attn\n",
    "# EncoderLayer takes 4 params (AttentionLayer, d_model, n_hidden = n_hidden or 4*d_model, dropout=0.1, activation=\"relu\") -> returns x, attns\n",
    "# Attn takes 5 params (mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False)\n",
    "\n",
    "\n",
    "\n",
    "#  d_model=512; n_heads=8; e_layers=3; d_ff=512; n_features = 9; dropout=0.0; attn='full'; activation='gelu'; ; distil=False; mix=True;\n",
    "        \n",
    "# We need to change the gelu and relu to allow for Sigmoid and Tanh in the Encoder\n",
    "# output_attention = Flase because Encoder returns  (enc_out, attns)\n",
    "# d_ff is like d_hidden\n",
    "\n",
    "\n",
    "# Before Encoder we need to feed a positional encoding and linear layer to map the dimensions to that encoder\n",
    "# After Encoder we need to remap the dimensiosn \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "plastic-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=10, d_model=512, n_heads=8, n_hidden=512, e_layers=3, dropout=0.0,seq_length = window_size,\n",
    "                 attn='prob', mask_flag = True, factor=5,activation='gelu',distil=False, mix=False, output_attention = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in=n_features, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.InputLinear = nn.Linear(n_features, d_model)\n",
    "        self.attn = attn\n",
    "        self.d_model = d_model\n",
    "        Attn = ProbAttention if attn=='prob' else FullAttention\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(Attn(mask_flag=mask_flag, factor=factor, attention_dropout=dropout, output_attention=output_attention), \n",
    "                                d_model, n_heads, mix),\n",
    "                    d_model,\n",
    "                    n_hidden,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            [ConvLayer(d_model) for l in range(e_layers-1)] if distil else None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------#\n",
    "        \n",
    "        #-----------------------------------------------------------------------------------------------------------------------------#\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_features)\n",
    "        self.init_weights()\n",
    "        self.activation= nn.Sigmoid()\n",
    "        self.end_conv1 = nn.Conv1d(in_channels=102, out_channels=seq_length, kernel_size=1, bias=True)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src, attn_mask): #attn_mask is one mask that provides everything for us\n",
    "#         src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.value_embedding(src) + self.position_embedding(src) # + self.temporal_embedding(x_mark)\n",
    "        src = self.dropout(src)\n",
    "        \n",
    "        output,attns = self.encoder(src,attn_mask) #attn_mask is being passed to FullAttention or ProbAttention\n",
    "\n",
    "#         output = self.end_conv1(output)\n",
    "        \n",
    "        output = self.OutputLinear(output)\n",
    "        output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-artwork",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "endless-directive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect(),torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "active-spectrum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 8.7500e-01],\n",
       "         [1.6372e-02, 9.5754e-03, 1.3100e-02, 2.5107e-02, 2.1450e-02, 1.2708e-02,\n",
       "          0.0000e+00, 4.2485e-03, 1.2832e-05, 7.5000e-01],\n",
       "         [1.0121e-03, 9.8252e-03, 1.3233e-02, 1.8654e-02, 1.8484e-02, 9.2688e-03,\n",
       "          0.0000e+00, 3.8487e-04, 3.0931e-04, 6.2500e-01],\n",
       "         [7.3416e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 2.5982e-04, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]]),\n",
       " tensor([1., 1., 1., 1., 0.], dtype=torch.float64),\n",
       " tensor([False, False, False,  True,  True]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_train_X[10][:5],real_train_flags[10][:5],real_train_masks[10][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reflected-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,target,mask_sample =iter(train_dataloader).next()\n",
    "# prepare_attn_mask(mask_sample)[0][:5],x[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-newport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "married-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------Epoch1-----------------------------\n",
      "Batch 50/490\n",
      "Batch 100/490\n",
      "Batch 150/490\n",
      "Batch 200/490\n",
      "Batch 250/490\n",
      "Batch 300/490\n",
      "Batch 350/490\n",
      "Batch 400/490\n",
      "Batch 450/490\n",
      "Epoch 1 Loss is 0.4149412318379903\n",
      "Epoch 1 - Time (in minutes) is 0:03:34.560427\n",
      "--------------------------Epoch2-----------------------------\n",
      "Batch 50/490\n",
      "Batch 100/490\n",
      "Batch 150/490\n",
      "Batch 200/490\n",
      "Batch 250/490\n",
      "Batch 300/490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99a6b16280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/yelnady/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/rhome/yelnady/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b80aac0d2367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All Epochs Loss is\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_epochs_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-b80aac0d2367>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_attn_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mY_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m#--------------------------------------------LOSS MSE---------------------------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmse_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-db1e1ba30e32>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, attn_mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#attn_mask is being passed to FullAttention or ProbAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#         output = self.end_conv1(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/DoppelGANger/Informer/models/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mattn_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mattns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/DoppelGANger/Informer/models/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#     attn_mask = attn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# ))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         new_x, attn = self.attention(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/DoppelGANger/Informer/models/attn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         out, attn = self.inner_attention(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/DoppelGANger/Informer/models/attn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mL_Q\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mL_Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mscores_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prob_QK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mU_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# add scale factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/DoppelGANger/Informer/models/attn.py\u001b[0m in \u001b[0;36m_prob_QK\u001b[0;34m(self, Q, K, sample_k, n_top)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# use the reduced Q to calculate Q_K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         Q_reduce = Q[torch.arange(B)[:, None, None],\n\u001b[0m\u001b[1;32m     64\u001b[0m                      \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                      M_top, :] # factor*ln(L_q)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MyInformer().to(device)\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "def train(model,train_dataloader,n_epochs):\n",
    "    time_all = time()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  \n",
    "    losses = []\n",
    "    all_epochs_loss = []\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('--------------------------Epoch{}-----------------------------'.format(epoch+1))\n",
    "        time0 = time()\n",
    "        one_epoch_loss = []\n",
    "        for idx,(X,target,padding_mask) in enumerate(train_dataloader):\n",
    "\n",
    "            optimizer.zero_grad(),gc.collect(),torch.cuda.empty_cache()\n",
    "            attn_mask = prepare_attn_mask(padding_mask)\n",
    "\n",
    "            Y_predicted = model(X.to(device),attn_mask)\n",
    "            #--------------------------------------------LOSS MSE---------------------------------------------------#\n",
    "            mse_loss = nn.MSELoss(reduction='none')\n",
    "            loss = mse_loss(Y_predicted, target.to(device))\n",
    "            \n",
    "            \n",
    "            # 1- Use reduction='none' loss, and calculate MSE for the first 9 features only\n",
    "            # 2- Sum the loss across features -> (B,S)\n",
    "            # 3- Unsqueeze to use bmm -> loss: (B,S,1) , ~padding_mask.float(): (B,S,1)\n",
    "            # 4- Transpose loss, and bmm(loss,padding_mask) -> (B,1,1)\n",
    "            # 5- Calculate mean or sum of the batch losses\n",
    "            loss = torch.sum(loss,2)\n",
    "            padding_mask = (~padding_mask).unsqueeze(2).float().to(device)\n",
    "            loss = loss.unsqueeze(2).permute(0,2,1)\n",
    "            \n",
    "            loss = torch.bmm(loss,padding_mask).mean()\n",
    "            loss.backward()            \n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            one_epoch_loss.append(loss.item())\n",
    "        \n",
    "           \n",
    "            #------------------------------------------END LOSS-----------------------------------------------------#\n",
    "            del X\n",
    "            del target\n",
    "            \n",
    "            if ((idx+1)%50==0):\n",
    "                print(\"Batch {}/{}\".format(idx+1,len(train_dataloader)))\n",
    "        if ((epoch+1)%100==0):\n",
    "            torch.save(model.state_dict(), 'class_all_weights_informer_flags1')\n",
    "\n",
    "        print(\"Epoch {} Loss is {}\".format(epoch+1,np.mean(one_epoch_loss)))\n",
    "        print(\"Epoch {} - Time (in minutes) is {}\".format(epoch+1,timedelta(seconds=(time()-time0))))\n",
    "        all_epochs_loss.append(np.mean(one_epoch_loss))\n",
    "    \n",
    "    print(\"Total Time (in minutes) is {}\".format( timedelta(seconds=(time()-time_all))))\n",
    "    print(\"All Epochs Loss is\\n\",all_epochs_loss)\n",
    "    \n",
    "train(model,train_dataloader,n_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob gives 0.282603288663466 in 0:08:23 minutes \n",
    "# full gives 0.13638 in 0:10:14\n",
    "\n",
    "# Full gives 0.1431730 using just n_layers = 3 in just 0:07:09 minutes\n",
    "#prob gives 0.1298 using  just n_layers = 3 in just 0:07:11.3\n",
    "\n",
    "\n",
    "# Using distill and 3 layers only I got -> 1.43092 then 0.078535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'class_all_weights_informer_flags1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_flag doesn't provide anything related to our paddings, but we can provide our own matrix\n",
    "# The weight mask, which is the combination of the padding and causal masks, is used to know which positions to fill with \n",
    "# −∞ before computing the softmax, which will be zero after it.\n",
    "\n",
    "# first src_mask is used to block specific positions from attending and then key_padding_mask is used to block attending to pad tokens.\n",
    "\n",
    "\n",
    "# In ProbAttention or FullAttention, if mask_flag=True and attn_mask {given in encoder forward} = None --> Informer makes Attention Matrix Mask for you\n",
    "#                                  , if mask_flag=True and given mask --> Informer just converts True values to (-np.inf)\n",
    "#                                  , if mask_flag=False doesn't care about any type of masks at all\n",
    "\n",
    "#IMPORTANT -> We need just to send a attn_mask where False is Important, and True to ignore and it will be set to (-np.inf) by mask_flag=True\n",
    "\n",
    "# torch.masked_fill_(matrix,value): Fills elements of tensor with value where mask is True. \n",
    "\n",
    "# Mask Shape -> mask_shape = [B, 1, L, L] -> [B, n_heads, L, L]\n",
    "\n",
    "# The mask we provide need to have an attribute called mask as done in utils/masking so I build class MyProbMask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-russian",
   "metadata": {},
   "source": [
    "## We will pass everything ready in the attn_mask from EncoderLayer.forward() -> AttentionLayer.forward() -> ProbAttention.forward(attn_mask) -> _update_context(attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-encounter",
   "metadata": {},
   "source": [
    "- output_seq_len=label_len+pred_len because in decoder we have label_len is the history provided to be the X_token as in Figure1, then we tell it to predict the following lengths, with a total of output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to see difference between PropAttention and FullAttention\n",
    "# We need to know the difference implementations for attn_matrix in Prob and Full (Currently using Full)\n",
    "# We need to see why distill = False, makes the dimensions correct and ignores the conv layers\n",
    "\n",
    "# We need to understand factor and mix params\n",
    "# We need to know differnce between Encoder  and EncoderStack\n",
    "\n",
    "# Current Implementation -> No Linear Layer at all! and uses Full Attention which causes problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
