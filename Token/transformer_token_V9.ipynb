{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f5ad86-4248-4226-943d-0bc26a03e80e",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a618acb-32dc-4450-a211-32bba60b2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "from datetime import timedelta\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics.functional as FM\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "sys.path.append('../DG/gan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddef74-d62f-43c4-b7d0-cd6e45ac955b",
   "metadata": {},
   "source": [
    "# Loading Real Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f63d9569-72bc-46f5-987f-64a4d1dbb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_class(X,Y_labels,flag,class_label):\n",
    "    indices_class_label = np.where(Y_labels==class_label)\n",
    "    return X[indices_class_label], Y_labels[indices_class_label], flag[indices_class_label] \n",
    "    \n",
    "def get_n_samples(X,Y_labels,flag,n_samples):\n",
    "    randomList = random.sample(range(0, Y_labels.shape[0]), n_samples)\n",
    "    return X[randomList], Y_labels[randomList], flag[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. \n",
    "            # So we do remove those ones by converting them to zeros, then return only non-zero flags indices\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y_labels,flag):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y_labels[indices_non_zero], flag[indices_non_zero]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f052e082-0bba-43c0-9cab-0eab5f14d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('../data/google/data_train_reduced.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "real_train_X,real_train_Y_labels,real_train_flags = remove_zero_datapoints(real_train_X,real_train_Y_labels,real_train_flags)\n",
    "\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()\n",
    "\n",
    "real_train_masks = real_train_flags == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e442d72-09bb-44db-9d9f-877d37b5a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_real = np.load('../data/google/data_train_val.npz')\n",
    "\n",
    "real_val_X = torch.from_numpy(val_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_val_Y = torch.from_numpy(val_real['data_attribute']) #[50000,4]\n",
    "real_val_Y_labels = torch.argmax(real_val_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_val_flags = torch.from_numpy(val_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "real_val_X,real_val_Y_labels,real_val_flags = remove_zero_datapoints(real_val_X,real_val_Y_labels,real_val_flags)\n",
    "\n",
    "real_val_masks = real_val_flags == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d73b10f-6a20-4101-a0c3-ea0e724f4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real = np.load('../data/google/data_test_reduced.npz')\n",
    "\n",
    "real_test_X = torch.from_numpy(test_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_test_Y = torch.from_numpy(test_real['data_attribute']) #[50000,4]\n",
    "real_test_Y_labels = torch.argmax(real_test_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_test_flags = torch.from_numpy(test_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "real_test_X,real_test_Y_labels,real_test_flags = remove_zero_datapoints(real_test_X,real_test_Y_labels,real_test_flags)\n",
    "\n",
    "real_test_masks = real_test_flags == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13146a91-fb12-4c1f-bae7-bf651e107b39",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55d3bd8-0106-496b-97ad-9ca611872d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = real_train_X.size(0)\n",
    "S = real_train_X.size(1)\n",
    "E = real_train_X.size(2)\n",
    "\n",
    "# 1- Shift the targets\n",
    "Input_shifted = real_train_X[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1,E))\n",
    "targets = torch.cat((Input_shifted,Zero_at_the_end),1) # real_train_X shifted to the left one timestep\n",
    "\n",
    "targets=  targets[:,:400]\n",
    "real_train_masks = real_train_masks[:,:400]\n",
    "real_train_X = real_train_X[:,:400]\n",
    "real_train_flags = real_train_flags[:,:400]\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()\n",
    "\n",
    "\n",
    "S = real_train_X.size(1)\n",
    "\n",
    "params_dataloader = {'shuffle': True,'num_workers':8 ,'batch_size':128} # No need to shuffle rn, they are all the same class\n",
    "# \"num_workers\" is how many subprocesses to use for data loading.\n",
    "dataset = torch.utils.data.TensorDataset(real_train_X, targets, real_train_lengths, real_train_masks)\n",
    "train_dataloader  = torch.utils.data.DataLoader(dataset, **params_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41edd324-ba94-4101-bbe8-bea5a2ef2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Dataset and DataLoader \n",
    "\n",
    "B = real_val_X.size(0)\n",
    "S = real_val_X.size(1)\n",
    "E = real_val_X.size(2)\n",
    "\n",
    "Input_shifted = real_val_X[:,1:]\n",
    "Zero_at_the_end = torch.zeros((B,1,E))\n",
    "targets = torch.cat((Input_shifted,Zero_at_the_end),1) # real_train_X shifted to the left one timestep\n",
    "\n",
    "targets=  targets[:,:400]\n",
    "real_val_masks = real_val_masks[:,:400]\n",
    "real_val_X = real_val_X[:,:400]\n",
    "\n",
    "real_val_flags = real_val_flags[:,:400]\n",
    "real_val_lengths = torch.sum(real_val_flags,1).long()\n",
    "\n",
    "S = real_val_X.size(1)\n",
    "\n",
    "params_dataloader = {'shuffle': False,'num_workers':8 ,'batch_size':128} # No need to shuffle rn, they are all the same class\n",
    "dataset = torch.utils.data.TensorDataset(real_val_X, targets, real_val_lengths, real_val_masks)\n",
    "val_dataloader  = torch.utils.data.DataLoader(dataset, **params_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a66c26-3e0a-44e5-9c89-7075e49907cc",
   "metadata": {},
   "source": [
    "# TST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f896001-4f2c-4818-9525-e59267dfb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e147f88-9c5e-468e-bfb2-6e674538d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_features=9, d_model=256, n_heads=8, n_hidden=256, n_layers=8, dropout=0.0, S=400):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Time Series Transformer Model'\n",
    "        self.InputLinear = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_features) # The output of the encoder is similar to the input of the encoder, both are (B,S,d_model)\n",
    "        self.init_weights()\n",
    "     \n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e6)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask,padding_mask):\n",
    "        src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask,padding_mask)\n",
    "        output = self.OutputLinear(output.permute(1,0,2))\n",
    "        return output \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        X,target,lengths,padding_mask = batch\n",
    "        src_mask = self.generate_square_subsequent_mask(S).cuda()\n",
    "        X = X.permute(1,0,2)\n",
    "        class_probs  = self(X,src_mask,padding_mask)\n",
    "        lengths -=1\n",
    "        loss = nn.CrossEntropyLoss()(class_probs, lengths )\n",
    "        \n",
    "        return {'loss': loss,} # will call loss.backward() on what we return exactly. \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        if((self.current_epoch+1)%100==0):\n",
    "            torch.save(self.state_dict(), 'W_transformer_token_V9')\n",
    "        print(\"Epoch Loss:\",torch.stack([x[\"loss\"] for x in outputs]).mean().item())\n",
    "\n",
    "    # Lightning disables gradients, puts model in eval mode, and does everything needed for validation.\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X,target,lengths,padding_mask = batch\n",
    "        src_mask = self.generate_square_subsequent_mask(S).cuda()\n",
    "        X = X.permute(1,0,2)\n",
    "        class_probs  = self(X,src_mask,padding_mask)\n",
    "        lengths -=1\n",
    "        loss = nn.CrossEntropyLoss()(class_probs, lengths )\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        return {'val_loss': loss,} # We may return the predictions themselves\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        print(\"Validation Loss:\",torch.stack([x[\"val_loss\"] for x in outputs]).mean().item())\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164179e2-73f2-48af-a11a-5faa94faaa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TimeSeriesTransformer() \n",
    "ck = torch.load('../lightning_logs/version_19/checkpoints/epoch=399-step=76799.ckpt')['state_dict']\n",
    "model.load_state_dict(ck)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.OutputLinear  = nn.Sequential(nn.Flatten(1),nn.Linear(256*400,400) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b467803-ae24-4f4f-8942-42fd65352924",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4d0416-a2b0-4b24-911a-5d6cefcbfe59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (InputLinear): Linear(in_features=9, out_features=256, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (7): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (OutputLinear): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=102400, out_features=400, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck = torch.load('lightning_logs/version_102/checkpoints/epoch=2-step=575.ckpt')['state_dict']\n",
    "model.load_state_dict(ck)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a11479f3-a1c3-4b14-9cc8-0bb3cbc78563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0034, 0.0048, 0.0110,  ..., 0.0000, 0.0021, 0.0001],\n",
       "         [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0471, 0.0100, 0.0145,  ..., 0.0000, 0.0040, 0.0020],\n",
       "         [0.0228, 0.0181, 0.0214,  ..., 0.0000, 0.0040, 0.0003],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_train_X[3400:3600][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b49c5487-880e-4887-91fc-d1ccef558e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = model.generate_square_subsequent_mask(S)\n",
    "x = torch.cat((real_train_X[2400:2600,:2],torch.zeros((200,398,9))),1)\n",
    "padding_mask = torch.cat((real_train_masks[2400:2600,:2],torch.ones((200,398),dtype=torch.bool)),1)\n",
    "\n",
    "# x= real_train_X[2400:2600]\n",
    "# padding_mask = real_train_masks[2400:2600]\n",
    "testt = model(real_train_X[3400:3600].permute(1,0,2),None,real_train_masks[3400:3600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b7e313d-a948-47c8-bc72-99918eda6f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) tensor(3)\n",
      "tensor(3) tensor(4)\n",
      "tensor(2) tensor(3)\n",
      "tensor(3) tensor(5)\n",
      "tensor(8) tensor(9)\n",
      "tensor(1) tensor(2)\n",
      "tensor(8) tensor(9)\n",
      "tensor(76) tensor(4)\n",
      "tensor(18) tensor(18)\n",
      "tensor(2) tensor(4)\n",
      "tensor(5) tensor(6)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(3) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(15) tensor(15)\n",
      "tensor(5) tensor(5)\n",
      "tensor(2) tensor(3)\n",
      "tensor(266) tensor(5)\n",
      "tensor(3) tensor(4)\n",
      "tensor(95) tensor(99)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(5) tensor(5)\n",
      "tensor(2) tensor(3)\n",
      "tensor(8) tensor(10)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(6) tensor(6)\n",
      "tensor(46) tensor(45)\n",
      "tensor(8) tensor(9)\n",
      "tensor(8) tensor(9)\n",
      "tensor(5) tensor(25)\n",
      "tensor(4) tensor(5)\n",
      "tensor(1) tensor(2)\n",
      "tensor(15) tensor(17)\n",
      "tensor(1) tensor(2)\n",
      "tensor(18) tensor(19)\n",
      "tensor(8) tensor(9)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(4)\n",
      "tensor(15) tensor(17)\n",
      "tensor(2) tensor(3)\n",
      "tensor(52) tensor(59)\n",
      "tensor(5) tensor(7)\n",
      "tensor(2) tensor(3)\n",
      "tensor(8) tensor(9)\n",
      "tensor(2) tensor(3)\n",
      "tensor(1) tensor(2)\n",
      "tensor(11) tensor(10)\n",
      "tensor(40) tensor(30)\n",
      "tensor(21) tensor(22)\n",
      "tensor(8) tensor(8)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(4)\n",
      "tensor(8) tensor(9)\n",
      "tensor(2) tensor(4)\n",
      "tensor(2) tensor(3)\n",
      "tensor(15) tensor(16)\n",
      "tensor(11) tensor(11)\n",
      "tensor(50) tensor(51)\n",
      "tensor(6) tensor(7)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(8) tensor(9)\n",
      "tensor(1) tensor(2)\n",
      "tensor(5) tensor(6)\n",
      "tensor(2) tensor(3)\n",
      "tensor(5) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(3)\n",
      "tensor(8) tensor(9)\n",
      "tensor(2) tensor(3)\n",
      "tensor(273) tensor(288)\n",
      "tensor(76) tensor(51)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(23) tensor(30)\n",
      "tensor(15) tensor(17)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(3)\n",
      "tensor(15) tensor(13)\n",
      "tensor(2) tensor(3)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(50) tensor(51)\n",
      "tensor(11) tensor(75)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(4)\n",
      "tensor(2) tensor(3)\n",
      "tensor(39) tensor(30)\n",
      "tensor(5) tensor(6)\n",
      "tensor(1) tensor(2)\n",
      "tensor(8) tensor(9)\n",
      "tensor(3) tensor(5)\n",
      "tensor(11) tensor(13)\n",
      "tensor(3) tensor(5)\n",
      "tensor(8) tensor(8)\n",
      "tensor(8) tensor(9)\n",
      "tensor(21) tensor(21)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(3)\n",
      "tensor(8) tensor(9)\n",
      "tensor(4) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(3)\n",
      "tensor(2) tensor(3)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(3)\n",
      "tensor(9) tensor(10)\n",
      "tensor(7) tensor(7)\n",
      "tensor(2) tensor(4)\n",
      "tensor(11) tensor(20)\n",
      "tensor(2) tensor(3)\n",
      "tensor(2) tensor(4)\n",
      "tensor(5) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(8) tensor(9)\n",
      "tensor(2) tensor(3)\n",
      "tensor(8) tensor(9)\n",
      "tensor(21) tensor(21)\n",
      "tensor(1) tensor(2)\n",
      "tensor(23) tensor(10)\n",
      "tensor(8) tensor(10)\n",
      "tensor(7) tensor(9)\n",
      "tensor(2) tensor(2)\n",
      "tensor(8) tensor(8)\n",
      "tensor(40) tensor(43)\n",
      "tensor(5) tensor(5)\n",
      "tensor(96) tensor(9)\n",
      "tensor(4) tensor(5)\n",
      "tensor(11) tensor(13)\n",
      "tensor(2) tensor(3)\n",
      "tensor(76) tensor(89)\n",
      "tensor(1) tensor(2)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(4)\n",
      "tensor(3) tensor(4)\n",
      "tensor(18) tensor(17)\n",
      "tensor(5) tensor(5)\n",
      "tensor(2) tensor(3)\n",
      "tensor(2) tensor(3)\n",
      "tensor(2) tensor(3)\n",
      "tensor(8) tensor(9)\n",
      "tensor(8) tensor(9)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(11) tensor(12)\n",
      "tensor(2) tensor(4)\n",
      "tensor(5) tensor(7)\n",
      "tensor(14) tensor(14)\n",
      "tensor(1) tensor(2)\n",
      "tensor(7) tensor(8)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(4)\n",
      "tensor(9) tensor(10)\n",
      "tensor(2) tensor(4)\n",
      "tensor(1) tensor(2)\n",
      "tensor(8) tensor(9)\n",
      "tensor(5) tensor(6)\n",
      "tensor(5) tensor(4)\n",
      "tensor(8) tensor(9)\n",
      "tensor(1) tensor(2)\n",
      "tensor(8) tensor(9)\n",
      "tensor(5) tensor(5)\n",
      "tensor(2) tensor(5)\n",
      "tensor(5) tensor(6)\n",
      "tensor(2) tensor(3)\n",
      "tensor(8) tensor(8)\n",
      "tensor(2) tensor(3)\n",
      "tensor(5) tensor(5)\n",
      "tensor(8) tensor(7)\n",
      "tensor(1) tensor(2)\n",
      "tensor(2) tensor(3)\n",
      "tensor(2) tensor(4)\n",
      "tensor(5) tensor(4)\n",
      "tensor(7) tensor(9)\n",
      "tensor(2) tensor(4)\n",
      "tensor(11) tensor(10)\n",
      "tensor(8) tensor(8)\n",
      "tensor(364) tensor(9)\n",
      "tensor(2) tensor(4)\n",
      "tensor(11) tensor(12)\n",
      "tensor(4) tensor(5)\n",
      "tensor(5) tensor(5)\n",
      "tensor(5) tensor(14)\n",
      "tensor(5) tensor(7)\n",
      "tensor(1) tensor(2)\n",
      "tensor(8) tensor(9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(F.softmax(testt).argmax(1),real_train_lengths[3400:3600]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498291b-f99f-4f99-902b-4bd5cf178d40",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22afa8-9519-4be3-a0e6-64da240005c0",
   "metadata": {},
   "source": [
    "**Notes on EarlyStopping:**\n",
    "- The EarlyStopping callback runs at the **end of every validation epoch**, which, under the default configuration, happen after **every training epoch**.\n",
    "-  However, the frequency of validation can be modified by setting various parameters in the Trainer, for example **check_val_every_n_epoch and val_check_interval**.\n",
    "- Note that the **patience** parameter counts the number of **validation epochs with no improvement**, and **not the number of training epochs**. \n",
    "    - Therefore, with parameters **check_val_every_n_epoch=10 and patience=3**, the trainer will perform at least **40 training epochs before being stopped**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a2c98b-72bd-483a-8c15-542e2f5f6231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | InputLinear         | Linear             | 2.6 K \n",
      "1 | positional_encoding | PositionalEncoding | 0     \n",
      "2 | transformer_encoder | TransformerEncoder | 3.2 M \n",
      "3 | OutputLinear        | Sequential         | 41.0 M\n",
      "-----------------------------------------------------------\n",
      "41.0 M    Trainable params\n",
      "3.2 M     Non-trainable params\n",
      "44.1 M    Total params\n",
      "176.517   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0375a25387ba40f5ba3810098d88d888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fa08014e3c8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/yelnady/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/rhome/yelnady/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time (in minutes) is 0:00:48.902175\n",
      "/rdata/yelnady/DoppelGANger/Token/lightning_logs/version_100/checkpoints/epoch=0-step=188.ckpt\n"
     ]
    }
   ],
   "source": [
    "# RuntimeError: CUDA error: device-side assert triggered --> The problem it needs to be 0-399 not 1-400\n",
    "def main():\n",
    "    # pl.seed_everything(42, workers=True) --> sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "    time_all = time()\n",
    "\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss',patience=5, verbose=False, mode='min')\n",
    "    checkpoint_callback = ModelCheckpoint()\n",
    "#     trainer = pl.Trainer(gpus=2,max_epochs=400, progress_bar_refresh_rate=50,accelerator ='ddp',\n",
    "#                         callbacks=[early_stop_callback,checkpoint_callback]\n",
    "#                          ,plugins=DDPPlugin(find_unused_parameters=False,check_val_every_n_epoch=2))\n",
    "    \n",
    "    \n",
    "    trainer = pl.Trainer(gpus=1,max_epochs=100, progress_bar_refresh_rate=50,check_val_every_n_epoch=3,\n",
    "                        callbacks=[checkpoint_callback],)\n",
    "    trainer.fit(model,train_dataloader,val_dataloader)\n",
    "    print(\"Total Time (in minutes) is {}\".format( timedelta(seconds=(time()-time_all))))\n",
    "    print(checkpoint_callback.best_model_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bee552c-0365-4425-a262-692d0185f5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
