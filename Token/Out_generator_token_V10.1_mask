cuda:1
total    : 50962169856
free     : 22557425664
used     : 28404744192

generator_token_V10.1.py:267: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  if F.softmax(nlp_predicted.squeeze()).argmax(1)[-1].item() == 399:
0/8062
100/8062
200/8062
300/8062
400/8062
500/8062
600/8062
700/8062
800/8062
900/8062
1000/8062
1100/8062
1200/8062
1300/8062
1400/8062
1500/8062
1600/8062
1700/8062
1800/8062
1900/8062
2000/8062
2100/8062
2200/8062
2300/8062
2400/8062
2500/8062
2600/8062
2700/8062
2800/8062
2900/8062
3000/8062
3100/8062
3200/8062
3300/8062
3400/8062
3500/8062
3600/8062
3700/8062
3800/8062
3900/8062
4000/8062
4100/8062
4200/8062
4300/8062
4400/8062
4500/8062
4600/8062
4700/8062
4800/8062
4900/8062
5000/8062
5100/8062
5200/8062
5300/8062
5400/8062
5500/8062
5600/8062
5700/8062
5800/8062
5900/8062
6000/8062
6100/8062
6200/8062
6300/8062
6400/8062
6500/8062
6600/8062
6700/8062
6800/8062
6900/8062
7000/8062
7100/8062
7200/8062
7300/8062
7400/8062
7500/8062
7600/8062
7700/8062
7800/8062
7900/8062
8000/8062
0/10636
Traceback (most recent call last):
  File "generator_token_V10.1.py", line 334, in <module>
    generate_dataset(real_train_X2 ,real_train_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_train_X2.size(0),max_length=max_length)
  File "generator_token_V10.1.py", line 262, in generate_dataset
    Y_predicted, nlp_predicted = model(datapoint.to(device),src_mask.to(device),mask.to(device))
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "generator_token_V10.1.py", line 190, in forward
    output = self.transformer_encoder(src, src_mask,padding_mask)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 195, in forward
    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 320, in forward
    src2 = self.self_attn(src, src, src, attn_mask=src_mask,
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1031, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 5082, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 4825, in _scaled_dot_product_attention
    attn = torch.bmm(q, k.transpose(-2, -1))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 47.46 GiB total capacity; 15.84 GiB already allocated; 17.44 MiB free; 17.19 GiB reserved in total by PyTorch)
