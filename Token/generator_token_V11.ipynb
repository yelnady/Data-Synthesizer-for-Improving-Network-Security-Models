{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd804dce",
   "metadata": {},
   "source": [
    "F.softmax(nlp_predicted.squeeze())# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e4fde077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "import seaborn as sns\n",
    "from tensorboard import default\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "sys.path.append('DG/gan')\n",
    "import gc\n",
    "print(device)\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "42fe389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 50962169856\n",
      "free     : 49308696576\n",
      "used     : 1653473280\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafd34f",
   "metadata": {},
   "source": [
    "# Import Real Training Data to Generate New Data from it.\n",
    "\n",
    "### Actual Distribution\n",
    "- Class0: 6250\n",
    "- Class1: 16124\n",
    "- Class2: 21273\n",
    "- Class3: 5278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da9cee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_class(X,Y,flag,class_label): # (X, Y, and flag) are the whole dataset that is consisted of many classes, Y is NOT One-Hot Encoded\n",
    "    indices_class_label = np.where(Y==class_label)\n",
    "    X,Y,flag = X[indices_class_label], Y[indices_class_label], flag[indices_class_label] \n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]\n",
    "\n",
    "def get_n_samples(X,Y,flag,n_samples):\n",
    "    randomList = random.sample(range(0, Y.shape[0]), n_samples)\n",
    "    return X[randomList], Y[randomList], flag[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. --> So we do remove those ones by converting them to zeros, then remove from the list\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y,flag):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "98039037",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('../data/google/data_train_reduced.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "#------------------------------------------------------------------Loading One Class------------------------------------------------\n",
    "real_train_X, real_train_Y_labels, real_train_flags= remove_zero_datapoints(real_train_X, real_train_Y_labels, real_train_flags)\n",
    "\n",
    "# The pading mask need to be inverted \n",
    "\n",
    "real_train_masks = real_train_flags == 0 # True when padding, False when considering\n",
    "\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba90972c-62ee-494f-a86c-ab54bd864696",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_X_nlp = (real_train_X.mean(2)*10**3).int()\n",
    "max_num = len(real_train_X_nlp.unique())\n",
    "\n",
    "vocab = dict(zip(real_train_X_nlp.unique().tolist(), np.arange(max_num)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f295fac6-55dd-4eaa-85cd-40acb940da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(real_train_X_nlp)):\n",
    "    for t in range(len(real_train_X_nlp[i])):\n",
    "        real_train_X_nlp[i,t] = vocab[real_train_X_nlp[i,t].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a360ef2d-3daf-4934-a23f-fdb75944565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes_disc = real_train_X_nlp.max() + 3 # The 3 are for padding and for end token and for starting from zero\n",
    "# IF 245 + 3 = 248, then put 246 as end token, and 247 as padding\n",
    "for i,after_last in enumerate(real_train_lengths):\n",
    "    real_train_X_nlp[i,after_last]  = n_classes_disc - 2  \n",
    "    real_train_X_nlp[i,after_last+1:] = torch.full((2500 - after_last - 1,), n_classes_disc - 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1989627f-84a7-4f3c-858c-7915da0c3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes_disc = n_classes_disc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee02bed",
   "metadata": {},
   "source": [
    "# PyTorch Transformer Model\n",
    "\n",
    "- Later, we need to remove this from here and put in a separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e64930ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "54c4f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=9, d_model=512, n_heads=8, n_hidden=512, n_layers=8, dropout=0.0, S=400):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Time Series Transformer Model (Working Inshallah)'\n",
    "        self.InputLinear = nn.Embedding(n_classes_disc, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_classes_disc) #For each timestep, create a probability over 300 classes\n",
    "        self.init_weights()\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e6)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask,padding_mask):\n",
    "        src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask,padding_mask)\n",
    "        output = self.OutputLinear(output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c89ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TimeSeriesTransformer().to(device)\n",
    "\n",
    "# ck = torch.load('../lightning_logs/version_19/checkpoints/epoch=399-step=76799.ckpt')['state_dict']\n",
    "# model.load_state_dict(ck)\n",
    "\n",
    "model.load_state_dict(torch.load('W_transformer_token_V11_512'))\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "456d1b20-3158-4828-bcb9-7ce75883144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1222)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache(), gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7f2c8be4-f993-4f50-b816-9f773b542f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 248])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9c1e7ff-5b8c-454b-8927-b680bf78a2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.4160, 0.1035, 0.0685],\n",
       "        [0.6403, 0.2169, 0.1243]]),\n",
       "indices=tensor([[  0,  14,  15],\n",
       "        [  3, 246,   0]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.topk(3,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d0c",
   "metadata": {},
   "source": [
    "# Generating New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "91cd32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# The following is the generating part #################################\n",
    "\n",
    "# Returns: X (The data)\n",
    "# Returns: masks (e.g. [False,Flase,True,True,True,....,True]), False is the actual Data\n",
    "\n",
    "resulted_masks = []\n",
    "generated_dataset_Y=[]\n",
    "generated_dataset_X=[]\n",
    "\n",
    "def generate_dataset(X,Y,masks,n_seed,n_samples,max_length):\n",
    "    for n in range(n_samples):\n",
    "        \n",
    "        datapoint,y,mask = get_n_samples(X,Y,masks,n_samples=1) # The first 10 timesteps of just one sample\n",
    "        datapoint = datapoint[:,:n_seed] \n",
    "        datapoint_len = torch.sum(~mask) #Flip and count, you will get the actual length to generate likewise\n",
    "        mask = mask[:,:n_seed] \n",
    "        S = datapoint.size(1)\n",
    "        for t in range(max_length-n_seed): # Loop until 400 timesteps\n",
    "            src_mask = model.generate_square_subsequent_mask(S)\n",
    "            nlp_predicted = model(datapoint.to(device),src_mask.to(device),mask.to(device))\n",
    "            nlp_predicted = nlp_predicted.cpu()\n",
    "            probs = F.softmax(nlp_predicted.squeeze()) #Convert to probabilities\n",
    "            v,i = nlp_predicted.topk(2)   # Choose from the top k , returns values (probs) and indices (labels) \n",
    "            one_new_timestep = random.choice(i[0,-1]) #Choose from the last timestep\n",
    "            \n",
    "            if one_new_timestep == n_classes_disc-2:\n",
    "                print(S, datapoint_len )\n",
    "                datapoint = torch.cat((datapoint,torch.zeros((1,max_length-S))),1).cpu()# Pad remainings with zero\n",
    "                mask =  torch.cat((mask,torch.full((1,max_length-S),True)),1)\n",
    "                break\n",
    "            \n",
    "            datapoint = torch.cat((datapoint,one_new_timestep.unsqueeze(0).unsqueeze(0)),1) # add the forecasted timestep\n",
    "            mask = torch.cat((mask,torch.tensor([[False]])),1 )\n",
    "            S = datapoint.size(1)\n",
    "        resulted_masks.append(mask.numpy())\n",
    "        generated_dataset_X.append(datapoint.squeeze().detach().numpy())\n",
    "        generated_dataset_Y.append(y.item())\n",
    "        del mask\n",
    "        del datapoint\n",
    "        del one_new_timestep\n",
    "        gc.collect(),torch.cuda.empty_cache()\n",
    "        \n",
    "        if (n%100==0):\n",
    "            print('{}/{}'.format(n,n_samples))\n",
    "        if ((n+1)%1000==0):\n",
    "             np.savez('npz_transformer_token_V11',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n",
    "\n",
    "max_length = 400\n",
    "n_seed = 2\n",
    "# Padding Mask Fed here is the Mask where \"False is Real Data\", True is masked and ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a648f5c-83e8-4843-8530-e257e5144aff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor(11)\n",
      "0/2639\n",
      "29 tensor(3)\n",
      "11 tensor(2)\n",
      "64 tensor(6)\n",
      "5 tensor(2)\n",
      "4 tensor(4)\n",
      "7 tensor(4)\n",
      "3 tensor(4)\n",
      "2 tensor(2)\n",
      "5 tensor(4)\n",
      "4 tensor(21)\n",
      "4 tensor(3)\n",
      "9 tensor(10)\n",
      "5 tensor(5)\n",
      "2 tensor(2)\n",
      "3 tensor(2)\n",
      "4 tensor(2)\n",
      "5 tensor(2)\n",
      "6 tensor(4)\n",
      "3 tensor(3)\n",
      "3 tensor(3)\n",
      "3 tensor(4)\n",
      "3 tensor(4)\n",
      "23 tensor(2)\n",
      "9 tensor(4)\n",
      "26 tensor(9)\n",
      "4 tensor(3)\n",
      "5 tensor(5)\n",
      "24 tensor(19)\n",
      "6 tensor(2)\n",
      "3 tensor(4)\n",
      "6 tensor(5)\n",
      "9 tensor(7)\n",
      "4 tensor(8)\n",
      "4 tensor(2)\n",
      "7 tensor(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "real_train_X0 ,real_train_Y_labels0 ,padding_mask0= get_one_class(real_train_X_nlp ,real_train_Y_labels ,real_train_masks,0)\n",
    "real_train_X1 ,real_train_Y_labels1 ,padding_mask1= get_one_class(real_train_X_nlp ,real_train_Y_labels ,real_train_masks,1)\n",
    "real_train_X2 ,real_train_Y_labels2 ,padding_mask2= get_one_class(real_train_X_nlp ,real_train_Y_labels ,real_train_masks,2)\n",
    "real_train_X3 ,real_train_Y_labels3 ,padding_mask3= get_one_class(real_train_X_nlp ,real_train_Y_labels ,real_train_masks,3)\n",
    "\n",
    "\n",
    "# generate_dataset(real_train_X0 ,real_train_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_train_X0.size(0),max_length=max_length)\n",
    "# generate_dataset(real_train_X1 ,real_train_Y_labels1 ,padding_mask1,n_seed=n_seed,n_samples=real_train_X1.size(0),max_length=max_length)\n",
    "# generate_dataset(real_train_X2 ,real_train_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_train_X2.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X3 ,real_train_Y_labels3 ,padding_mask3,n_seed=n_seed,n_samples=real_train_X3.size(0),max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c81528-2a26-43a9-92c7-a13b65684ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to make sure everything is calculated correctly in softmax and argmax\n",
    "\n",
    "# x = torch.rand((2,3))\n",
    "# print(x)\n",
    "# print(F.softmax(x).argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ab8ea-f7f2-4c77-84e8-965c3a281831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# real_test_X0 ,real_test_Y_labels0 ,padding_mask0= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,0)\n",
    "# real_test_X1 ,real_test_Y_labels1 ,padding_mask1= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,1)\n",
    "# real_test_X2 ,real_test_Y_labels2 ,padding_mask2= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,2)\n",
    "# real_test_X3 ,real_test_Y_labels3 ,padding_mask3= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,3)\n",
    "\n",
    "\n",
    "# generate_dataset(real_test_X0 ,real_test_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_test_X0.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X1 ,real_test_Y_labels1 ,padding_mask1,n_seed=n_seed,n_samples=real_test_X1.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X2 ,real_test_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_test_X2.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X3 ,real_test_Y_labels3 ,padding_mask3,n_seed=n_seed,n_samples=real_test_X3.size(0),max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193017c-1930-4e3e-bdaa-29a9fcb36697",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('npz_transformer_token_V11',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474e225-9e29-41a4-a74c-36e2d292d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c638b2-11d3-4bd1-a128-1a225e530077",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect(),torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d6dec-bdbb-4961-956c-28eb2390b470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What is the average values of all samples in variable length sequence\n",
    "\n",
    "mean_of_samples = []\n",
    "for x,length in zip(real_train_X, real_train_lengths):\n",
    "    mean_of_samples.append(torch.sum(x)/(9*length)) # torch.Size([2500])\n",
    "    \n",
    "print(np.mean(mean_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e2e56-6e32-4d9c-b04f-d0eb8356742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(input) → Tensor\n",
    "# Returns the maximum value of all elements in the input tensor.\n",
    "\n",
    "torch.max(real_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71072b7-4b93-41d4-9894-c81bd6686485",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(real_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0674c9-e124-4dd3-99be-e7dc8ca16a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(real_train_X.flatten(), 'o', color='black');\n",
    "plt.xscale(\"log\")\n",
    "ax.set_title('CDF - Class all')\n",
    "ax.set_xlabel('The Sequence Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8cf9d-b707-430d-bf46-1f13e24d8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(real_train_X,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134539e-d658-4fa6-87f0-f75d99814867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(torch.mean(real_train_X,2).flatten(), 'o', color='black');\n",
    "plt.xscale(\"log\")\n",
    "ax.set_title('CDF - Class all')\n",
    "ax.set_xlabel('The Sequence Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6aa925-a487-46f4-b536-4cb11877f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([1,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d43f4-af58-47d6-abdb-bdb7c4e3c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
