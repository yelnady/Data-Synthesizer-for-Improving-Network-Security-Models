{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd804dce",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fde077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "import seaborn as sns\n",
    "from tensorboard import default\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "sys.path.append('DG/gan')\n",
    "import gc\n",
    "print(device)\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fe389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 50962169856\n",
      "free     : 50958106624\n",
      "used     : 4063232\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafd34f",
   "metadata": {},
   "source": [
    "# Import Real Training Data to Generate New Data from it.\n",
    "\n",
    "### Actual Distribution\n",
    "- Class0: 6250\n",
    "- Class1: 16124\n",
    "- Class2: 21273\n",
    "- Class3: 5278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9cee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_class(X,Y,flag,class_label): # (X, Y, and flag) are the whole dataset that is consisted of many classes, Y is NOT One-Hot Encoded\n",
    "    indices_class_label = np.where(Y==class_label)\n",
    "    X,Y,flag = X[indices_class_label], Y[indices_class_label], flag[indices_class_label] \n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]\n",
    "\n",
    "def get_n_samples(X,Y,flag,n_samples):\n",
    "    randomList = random.sample(range(0, Y.shape[0]), n_samples)\n",
    "    return X[randomList], Y[randomList], flag[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. --> So we do remove those ones by converting them to zeros, then remove from the list\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y,flag):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98039037",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('../data/google/data_train_reduced.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "#------------------------------------------------------------------Loading One Class------------------------------------------------\n",
    "real_train_X, real_train_Y_labels, real_train_flags= remove_zero_datapoints(real_train_X, real_train_Y_labels, real_train_flags)\n",
    "\n",
    "# The pading mask need to be inverted \n",
    "\n",
    "real_train_masks = real_train_flags == 0 # True when padding, False when considering\n",
    "\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7138d0-10ae-44d2-813a-b9fea7179f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes_disc = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c1b74c-0f7b-4367-ae59-ba626285eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "minn = 0\n",
    "maxx = 0.4\n",
    "\n",
    "real_train_X_nlp = ( real_train_X.mean(2) - minn) / (maxx - minn)\n",
    "\n",
    "# End Token to be in range (0.4 - 0.6)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85197249-80dc-4ea2-a6ba-831a7820d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_X_cat = torch.cat((real_train_X, real_train_X_nlp.unsqueeze(2)),2)\n",
    "real_train_X_cat = real_train_X_cat[:,:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3d7474-eda7-413c-b45e-a7abe525f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_real = np.load('../data/google/data_test_reduced.npz')\n",
    "\n",
    "# real_test_X = torch.from_numpy(test_real['data_feature']).float() #[50000, 2500, 9]\n",
    "# real_test_Y = torch.from_numpy(test_real['data_attribute']) #[50000,4]\n",
    "# real_test_Y_labels = torch.argmax(real_test_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "# real_test_flags = torch.from_numpy(test_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "# real_test_X,real_test_Y_labels,real_test_flags = remove_zero_datapoints(real_test_X,real_test_Y_labels,real_test_flags)\n",
    "\n",
    "# real_test_masks = real_test_flags == 0\n",
    "\n",
    "# real_test_lengths = torch.sum(real_test_flags,1).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee02bed",
   "metadata": {},
   "source": [
    "# PyTorch Transformer Model\n",
    "\n",
    "- Later, we need to remove this from here and put in a separate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64930ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c4f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=10, d_model=256, n_heads=8, n_hidden=256, n_layers=8, dropout=0.0, S=400):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Time Series Transformer Model'\n",
    "        self.InputLinear = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_features) # The output of the encoder is similar to the input of the encoder, both are (B,S,d_model)\n",
    "        self.init_weights()\n",
    "        self.activation = nn.Sigmoid() \n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e6)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask,padding_mask):\n",
    "        src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask,padding_mask)\n",
    "        output = self.OutputLinear(output)\n",
    "        output = self.activation(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c89ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TimeSeriesTransformer().to(device)\n",
    "\n",
    "# ck = torch.load('../lightning_logs/version_19/checkpoints/epoch=399-step=76799.ckpt')['state_dict']\n",
    "# model.load_state_dict(ck)\n",
    "\n",
    "model.load_state_dict(torch.load('W_transformer_token_V10.5'))\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b982638b-0585-4085-bb87-52adb27bf744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 2500\n",
    "# b = 2600\n",
    "\n",
    "# testt = model_classifier(real_train_X[a:b,:2].permute(1,0,2).to(device),None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9a9e4cd-6abf-4f34-893d-8e693d285639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i,j in zip(F.softmax(testt).argmax(1)+1,real_train_lengths[a:b]):\n",
    "#     print(i.item(),j.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456d1b20-3158-4828-bcb9-7ce75883144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f26e8808-3b45-4501-ae92-fe943ad9022a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d0c",
   "metadata": {},
   "source": [
    "# Generating New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91cd32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# The following is the generating part #################################\n",
    "\n",
    "# Returns: X (The data)\n",
    "# Returns: masks (e.g. [False,Flase,True,True,True,....,True]), False is the actual Data\n",
    "\n",
    "resulted_masks = []\n",
    "generated_dataset_Y=[]\n",
    "generated_dataset_X=[]\n",
    "\n",
    "def generate_dataset(X,Y,masks,n_seed,n_samples,max_length):\n",
    "    for n in range(n_samples):\n",
    "        \n",
    "        datapoint,y,mask = get_n_samples(X,Y,masks,n_samples=1) # The first 10 timesteps of just one sample\n",
    "        datapoint = datapoint[:,:n_seed] \n",
    "        datapoint_len = torch.sum(~mask) #Flip and count, you will get the actual length to generate likewise\n",
    "        mask = mask[:,:n_seed] \n",
    "        E = datapoint.size(2)\n",
    "        S = datapoint.size(1)\n",
    "        print('Actual Length is',datapoint_len)\n",
    "        for t in range(max_length-n_seed): # Loop until 400 timesteps\n",
    "            src_mask = model.generate_square_subsequent_mask(S)\n",
    "            Y_predicted = model(datapoint.to(device),src_mask.to(device),mask.to(device))\n",
    "            Y_predicted = Y_predicted.cpu()\n",
    "            one_new_timestep=Y_predicted[:,-1].unsqueeze(0)\n",
    "\n",
    "            if Y_predicted[:,-1,9:].squeeze().item() >= 0.4:\n",
    "                print(S, datapoint_len )\n",
    "                datapoint = torch.cat((datapoint,torch.zeros((1,max_length-S,E))),1).cpu()# Pad remainings with zero\n",
    "                mask =  torch.cat((mask,torch.full((1,max_length-S),True)),1)\n",
    "        \n",
    "                break\n",
    "            \n",
    "            datapoint = torch.cat((datapoint,one_new_timestep),1) # add the forecasted timestep\n",
    "            mask = torch.cat((mask,torch.tensor([[False]])),1 )\n",
    "            S = datapoint.size(1)\n",
    "        print('DONE')    \n",
    "        resulted_masks.append(mask.numpy())\n",
    "        generated_dataset_X.append(datapoint.squeeze().detach().numpy())\n",
    "        generated_dataset_Y.append(y.item())\n",
    "        del mask\n",
    "        del datapoint\n",
    "        del one_new_timestep\n",
    "        gc.collect(),torch.cuda.empty_cache()\n",
    "        \n",
    "        if (n%100==0):\n",
    "            print('{}/{}'.format(n,n_samples))\n",
    "        if ((n+1)%1000==0):\n",
    "             np.savez('npz_transformer_token_V10.5',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n",
    "\n",
    "max_length = 400\n",
    "n_seed = 2\n",
    "# Padding Mask Fed here is the Mask where \"False is Real Data\", True is masked and ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a648f5c-83e8-4843-8530-e257e5144aff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Length is tensor(7)\n",
      "0.0011712305713444948\n",
      "0.00118186604231596\n",
      "0.001158214290626347\n",
      "0.0011512701166793704\n",
      "0.0011501680128276348\n",
      "0.0011511610355228186\n",
      "0.0011528426548466086\n",
      "0.00115467538125813\n",
      "0.0011564393062144518\n",
      "0.0011580525897443295\n",
      "0.0011595014948397875\n",
      "0.0011607827618718147\n",
      "0.0011619155993685126\n",
      "0.0011629038490355015\n",
      "0.0011637782445177436\n",
      "0.0011645378544926643\n",
      "0.0011652152752503753\n",
      "0.0011658109724521637\n",
      "0.0011663350742310286\n",
      "0.0011667996877804399\n",
      "0.0011672098189592361\n",
      "0.0011675739660859108\n",
      "0.001167896669358015\n",
      "0.0011681803734973073\n",
      "0.0011684407945722342\n",
      "0.0011686660582199693\n",
      "0.001168865361250937\n",
      "0.0011690508108586073\n",
      "0.0011692139087244868\n",
      "0.0011693565174937248\n",
      "0.0011694823624566197\n",
      "0.0011695927241817117\n",
      "0.0011696997098624706\n",
      "0.0011697860900312662\n",
      "0.001169868977740407\n",
      "0.0011699364986270666\n",
      "0.0011700027389451861\n",
      "0.0011700596660375595\n",
      "0.001170107047073543\n",
      "0.00117015209980309\n",
      "0.0011701888870447874\n",
      "0.001170220086351037\n",
      "0.0011702446499839425\n",
      "0.001170269213616848\n",
      "0.0011702908668667078\n",
      "0.0011703070485964417\n",
      "0.0011703198542818427\n",
      "0.001170328352600336\n",
      "0.0011703382479026914\n",
      "0.00117034325376153\n",
      "0.0011703416239470243\n",
      "0.0011703483760356903\n",
      "0.001170347211882472\n",
      "0.0011703427881002426\n",
      "0.0011703427881002426\n",
      "0.0011703394120559096\n",
      "0.0011703271884471178\n",
      "0.0011703171767294407\n",
      "0.0011703165946528316\n",
      "0.0011703009949997067\n",
      "0.00117029482498765\n",
      "0.0011702837655320764\n",
      "0.0011702680494636297\n",
      "0.0011702579213306308\n",
      "0.001170243020169437\n",
      "0.0011702290503308177\n",
      "0.0011702222982421517\n",
      "0.0011701988987624645\n",
      "0.0011701894691213965\n",
      "0.0011701766634359956\n",
      "0.001170155475847423\n",
      "0.0011701398761942983\n",
      "0.001170119270682335\n",
      "0.0011701053008437157\n",
      "0.0011700920294970274\n",
      "0.0011700752656906843\n",
      "0.0011700596660375595\n",
      "0.0011700390605255961\n",
      "0.001170025672763586\n",
      "0.0011700061149895191\n",
      "0.0011699843453243375\n",
      "0.00116997049190104\n",
      "0.0011699519818648696\n",
      "0.0011699381284415722\n",
      "0.0011699197348207235\n",
      "0.0011699058813974261\n",
      "0.0011698841117322445\n",
      "0.0011698686284944415\n",
      "0.0011698523303493857\n",
      "0.0011698295129463077\n",
      "0.0011698172893375158\n",
      "0.0011698011076077819\n",
      "0.0011697810841724277\n",
      "0.0011697609443217516\n",
      "0.0011697475565597415\n",
      "0.0011697342852130532\n",
      "0.0011697141453623772\n",
      "0.0011697025038301945\n",
      "0.0011696829460561275\n",
      "0.0011696635046973825\n",
      "0.0011696495348587632\n",
      "0.0011696323053911328\n",
      "0.0011696100700646639\n",
      "0.0011696027358993888\n",
      "0.0011695805005729198\n",
      "0.0011695637367665768\n",
      "0.001169548137113452\n",
      "0.0011695320717990398\n",
      "0.0011695231078192592\n",
      "0.0011695045977830887\n",
      "0.0011694857385009527\n",
      "0.0011694657150655985\n",
      "0.0011694624554365873\n",
      "0.0011694439454004169\n",
      "0.0011694300919771194\n",
      "0.0011694167042151093\n",
      "0.0011694066924974322\n",
      "0.0011693788692355156\n",
      "0.001169373164884746\n",
      "0.0011693565174937248\n",
      "0.0011693432461470366\n",
      "0.0011693231062963605\n",
      "0.0011693042470142245\n",
      "0.001169308670796454\n",
      "0.0011692818952724338\n",
      "0.0011692752595990896\n",
      "0.0011692546540871263\n",
      "0.0011692384723573923\n",
      "0.0011692296247929335\n",
      "0.0011692083207890391\n",
      "0.0011692038970068097\n",
      "0.0011691960971802473\n",
      "0.001169176190160215\n",
      "0.0011691671097651124\n",
      "0.0011691482504829764\n",
      "0.0011691382387652993\n",
      "0.001169127062894404\n",
      "0.001169120310805738\n",
      "0.001169108203612268\n",
      "0.001169092021882534\n",
      "0.001169079216197133\n",
      "0.001169069204479456\n",
      "0.0011690596584230661\n",
      "0.0011690430110320449\n",
      "0.0011690319515764713\n",
      "0.00116901402361691\n",
      "0.001169007970020175\n",
      "0.001168995164334774\n",
      "0.001168979099020362\n",
      "0.0011689822422340512\n",
      "0.0011689623352140188\n",
      "0.0011689589591696858\n",
      "0.0011689243838191032\n",
      "0.001168929971754551\n",
      "0.001168917748145759\n",
      "0.0011689133243635297\n",
      "0.0011689065722748637\n",
      "0.0011688927188515663\n",
      "0.00116886873729527\n",
      "0.001168863265775144\n",
      "0.0011688520899042487\n",
      "0.0011688603553920984\n",
      "0.0011688309023156762\n",
      "0.0011688288068398833\n",
      "0.001168807502835989\n",
      "0.001168814254924655\n",
      "0.0011687974911183119\n",
      "0.0011687896912917495\n",
      "0.0011687662918120623\n",
      "0.0011687640799209476\n",
      "0.0011687518563121557\n",
      "0.0011687474325299263\n",
      "0.0011687480146065354\n",
      "0.0011687345104292035\n",
      "0.001168721355497837\n",
      "0.0011687073856592178\n",
      "0.0011687017977237701\n",
      "0.0011686906218528748\n",
      "0.0011686934158205986\n",
      "0.0011686728103086352\n",
      "0.0011686678044497967\n",
      "0.0011686549987643957\n",
      "0.0011686539510264993\n",
      "0.0011686417274177074\n",
      "0.001168630551546812\n",
      "0.0011686327634379268\n",
      "0.001168621121905744\n",
      "0.0011685993522405624\n",
      "0.0011685987701639533\n",
      "0.0011685803765431046\n",
      "0.0011685837525874376\n",
      "0.001168575370684266\n",
      "0.0011685671051964164\n",
      "0.0011685637291520834\n",
      "0.0011685414938256145\n",
      "0.001168542541563511\n",
      "0.0011685409117490053\n",
      "0.0011685281060636044\n",
      "0.0011685325298458338\n",
      "0.0011685180943459272\n",
      "0.0011685080826282501\n",
      "0.0011685008648782969\n",
      "0.0011684974888339639\n",
      "0.0011684936471283436\n",
      "0.0011684880591928959\n",
      "0.0011684696655720472\n",
      "0.001168475835584104\n",
      "0.0011684580240398645\n",
      "0.0011684562778100371\n",
      "0.0011684390483424067\n",
      "0.0011684352066367865\n",
      "0.0011684246128425002\n",
      "0.0011684224009513855\n",
      "0.0011684135533869267\n",
      "0.0011684157652780414\n",
      "0.001168407965451479\n",
      "0.0011683867778629065\n",
      "0.0011683923657983541\n",
      "0.0011683733901008964\n",
      "0.001168375601992011\n",
      "0.0011683623306453228\n",
      "0.0011683633783832192\n",
      "0.0011683651246130466\n",
      "0.001168350107036531\n",
      "0.0011683243792504072\n",
      "0.0011683455668389797\n",
      "0.0011683473130688071\n",
      "0.0011683200718834996\n",
      "0.0011683243792504072\n",
      "0.0011683121556416154\n",
      "0.0011683071497827768\n",
      "0.00116828887257725\n",
      "0.0011683021439239383\n",
      "0.0011682944605126977\n",
      "0.0011682843323796988\n",
      "0.0011682898038998246\n",
      "0.0011682772310450673\n",
      "0.0011682565091177821\n",
      "0.0011682737385854125\n",
      "0.0011682376498356462\n",
      "0.0011682548793032765\n",
      "0.0011682576732710004\n",
      "0.0011682542972266674\n",
      "0.0011682442855089903\n",
      "0.001168231014162302\n",
      "0.0011682320619001985\n",
      "0.0011682165786623955\n",
      "0.0011682159965857863\n",
      "0.0011682220501825213\n",
      "0.0011682031909003854\n",
      "0.0011682076146826148\n",
      "0.0011681908508762717\n",
      "0.001168182585388422\n",
      "0.0011681808391585946\n",
      "0.0011681714095175266\n",
      "0.0011681764153763652\n",
      "0.0011681659379974008\n",
      "0.001168169779703021\n",
      "0.001168153714388609\n",
      "0.0011681553442031145\n",
      "0.0011681581381708384\n",
      "0.0011681519681587815\n",
      "0.0011681591859087348\n",
      "0.0011681419564411044\n",
      "0.0011681197211146355\n",
      "0.0011681357864290476\n",
      "0.0011681107571348548\n",
      "0.0011681329924613237\n",
      "0.0011681219330057502\n",
      "0.0011681074975058436\n",
      "0.0011680746683850884\n",
      "0.001168101909570396\n",
      "0.0011680969037115574\n",
      "0.001168102491647005\n",
      "0.001168098533526063\n",
      "0.0011681097093969584\n",
      "0.0011680684983730316\n",
      "0.0011680740863084793\n",
      "0.0011680590687319636\n",
      "0.0011680640745908022\n",
      "0.0011680779280140996\n",
      "0.0011680752504616976\n",
      "0.0011680852621793747\n",
      "0.0011680428870022297\n",
      "0.0011680450988933444\n",
      "0.001168056856840849\n",
      "0.0011680456809699535\n",
      "0.0011680484749376774\n",
      "0.0011680196039378643\n",
      "0.0011680374154821038\n",
      "0.0011680306633934379\n",
      "0.0011680228635668755\n",
      "0.001168025191873312\n",
      "0.0011679994640871882\n",
      "0.001168011804111302\n",
      "0.0011680028401315212\n",
      "0.0011679884046316147\n",
      "0.0011679994640871882\n",
      "0.001168011804111302\n",
      "0.0011679945746436715\n",
      "0.0011680010939016938\n",
      "0.0011679672170430422\n",
      "0.0011679750168696046\n",
      "0.0011679844465106726\n",
      "0.0011679739691317081\n",
      "0.001167976763099432\n",
      "0.0011679617455229163\n",
      "0.0011679349699988961\n",
      "0.001167963957414031\n",
      "0.0011679460294544697\n",
      "0.0011679378803819418\n",
      "0.001167957321740687\n",
      "0.001167947193607688\n",
      "0.0011679495219141245\n",
      "0.0011679367162287235\n",
      "0.0011679243762046099\n",
      "0.0011679267045110464\n",
      "0.0011679255403578281\n",
      "0.0011679271701723337\n",
      "0.0011679105227813125\n",
      "0.001167899346910417\n",
      "0.0011679149465635419\n",
      "0.0011679071467369795\n",
      "0.00116790272295475\n",
      "0.0011679071467369795\n",
      "0.0011679149465635419\n",
      "0.0011679043527692556\n",
      "0.0011678938753902912\n",
      "0.0011678860755637288\n",
      "0.00116790272295475\n",
      "0.0011678826995193958\n",
      "0.0011678760638460517\n",
      "0.0011678849114105105\n",
      "0.001167892711237073\n",
      "0.0011678715236485004\n",
      "0.0011678694281727076\n",
      "0.00116787722799927\n",
      "0.0011678794398903847\n",
      "0.0011678548762574792\n",
      "0.0011678700102493167\n",
      "0.001167860347777605\n",
      "0.001167870475910604\n",
      "0.0011678292648866773\n",
      "0.0011678609298542142\n",
      "0.001167860347777605\n",
      "0.0011678816517814994\n",
      "0.0011678526643663645\n",
      "0.0011678504524752498\n",
      "0.0011678242590278387\n",
      "0.0011678282171487808\n",
      "0.0011678609298542142\n",
      "0.001167831476777792\n",
      "0.0011678336886689067\n",
      "0.001167838810943067\n",
      "0.0011678282171487808\n",
      "0.001167841488495469\n",
      "0.0011678531300276518\n",
      "0.0011678214650601149\n",
      "0.001167809241451323\n",
      "0.001167812617495656\n",
      "0.001167812617495656\n",
      "0.0011677981819957495\n",
      "0.0011678170412778854\n",
      "0.001167787122540176\n",
      "0.0011677970178425312\n",
      "0.0011678080772981048\n",
      "0.0011677637230604887\n",
      "0.0011677725706249475\n",
      "0.0011677981819957495\n",
      "0.0011677792062982917\n",
      "0.0011678059818223119\n",
      "0.0011677647707983851\n",
      "0.0011677943402901292\n",
      "0.001167751383036375\n",
      "0.0011677658185362816\n",
      "0.0011677425354719162\n",
      "0.0011677725706249475\n",
      "0.001167758135125041\n",
      "0.0011677704751491547\n",
      "0.0011677625589072704\n",
      "0.001167783630080521\n",
      "0.001167802605777979\n",
      "0.0011677859583869576\n",
      "0.00116778037045151\n",
      "0.001167773618362844\n",
      "0.0011677459115162492\n",
      "0.0011677381116896868\n",
      "0.0011677569709718227\n",
      "0.0011677441652864218\n",
      "0.0011677280999720097\n",
      "0.001167732523754239\n",
      "0.0011677613947540522\n",
      "0.0011677647707983851\n",
      "0.0011677079601213336\n",
      "0.0011677535949274898\n",
      "0.001167744747363031\n",
      "0.0011677403235808015\n",
      "0.0011677253060042858\n",
      "DONE\n",
      "0/3125\n",
      "Actual Length is tensor(7)\n",
      "0.001126650138758123\n",
      "0.0011550197377800941\n",
      "0.0012474338291212916\n",
      "0.0012534628622233868\n",
      "0.001237807096913457\n",
      "0.0012182582868263125\n",
      "0.0011999900452792645\n",
      "0.0011843194952234626\n",
      "0.001171351526863873\n",
      "0.0011607762426137924\n",
      "0.0011522317072376609\n",
      "0.001145361689850688\n",
      "0.00113984989002347\n",
      "0.0011354493908584118\n",
      "0.00113192992284894\n",
      "0.0011291664559394121\n",
      "0.0011270124232396483\n",
      "0.0011253682896494865\n",
      "0.0011241371976211667\n",
      "0.0011232482502236962\n",
      "0.001122647081501782\n",
      "0.001122277695685625\n",
      "0.0011221107561141253\n",
      "0.001122107612900436\n",
      "0.0011222338071092963\n",
      "0.0011224689660593867\n",
      "0.0011228042421862483\n",
      "0.0011232097167521715\n",
      "0.0011236810823902488\n",
      "0.0011241971515119076\n",
      "0.001124758506193757\n",
      "0.001125351176597178\n",
      "0.0011259699240326881\n",
      "0.0011266113724559546\n",
      "0.0011272652773186564\n",
      "0.0011279312893748283\n",
      "0.0011286045191809535\n",
      "0.001129281590692699\n",
      "0.0011299621546640992\n",
      "0.001130641670897603\n",
      "0.0011313252616673708\n",
      "0.0011320022167637944\n",
      "0.0011326710227876902\n",
      "0.0011333347065374255\n",
      "0.0011339923366904259\n",
      "0.001134646125137806\n",
      "0.0011352872243151069\n",
      "0.0011359212221577764\n",
      "0.0011365467216819525\n",
      "0.001137161860242486\n",
      "0.0011377708287909627\n",
      "0.0011383675737306476\n",
      "0.0011389527935534716\n",
      "0.0011395328911021352\n",
      "0.0011401001829653978\n",
      "0.0011406607227399945\n",
      "0.0011412090389057994\n",
      "0.001141744083724916\n",
      "0.0011422716779634356\n",
      "0.0011427950812503695\n",
      "0.0011433031177148223\n",
      "0.0011438025394454598\n",
      "0.0011442902032285929\n",
      "0.001144776470027864\n",
      "0.001145246671512723\n",
      "0.0011457103537395597\n",
      "0.0011461665853857994\n",
      "0.0011466160649433732\n",
      "0.001147050759755075\n",
      "0.0011474862694740295\n",
      "0.0011479066452011466\n",
      "0.0011483188718557358\n",
      "0.001148720970377326\n",
      "0.0011491248151287436\n",
      "0.0011495129438117146\n",
      "0.0011499034008011222\n",
      "0.001150278141722083\n",
      "0.0011506451992318034\n",
      "0.0011510069016367197\n",
      "0.0011513667413964868\n",
      "0.0011517121456563473\n",
      "0.0011520633706822991\n",
      "0.0011523980647325516\n",
      "0.0011527269380167127\n",
      "0.001153053599409759\n",
      "0.001153371063992381\n",
      "0.0011536858510226011\n",
      "0.0011539908591657877\n",
      "0.0011542943539097905\n",
      "0.0011545944726094604\n",
      "0.0011548871407285333\n",
      "0.0011551700299605727\n",
      "0.001155448378995061\n",
      "0.001155723468400538\n",
      "0.0011560011189430952\n",
      "0.001156264217570424\n",
      "0.0011565263848751783\n",
      "0.0011567814508453012\n",
      "0.0011570415226742625\n",
      "0.0011572906514629722\n",
      "0.0011575325625017285\n",
      "0.0011577692348510027\n",
      "0.0011580080026760697\n",
      "0.0011582373408600688\n",
      "0.00115847191773355\n",
      "0.0011586981127038598\n",
      "0.0011589167406782508\n",
      "0.0011591347865760326\n",
      "0.0011593495728448033\n",
      "0.0011595627292990685\n",
      "0.0011597676202654839\n",
      "0.001159972045570612\n",
      "0.0011601720470935106\n",
      "0.001160365529358387\n",
      "0.0011605556355789304\n",
      "0.0011607491178438067\n",
      "0.0011609414359554648\n",
      "0.0011611278168857098\n",
      "0.0011613087262958288\n",
      "0.001161486841738224\n",
      "0.0011616594856604934\n",
      "0.0011618370190262794\n",
      "0.0011620124569162726\n",
      "0.0011621752055361867\n",
      "0.0011623445898294449\n",
      "0.0011625029146671295\n",
      "0.0011626678751781583\n",
      "0.0011628263164311647\n",
      "0.0011629847576841712\n",
      "0.001163132139481604\n",
      "0.0011632922105491161\n",
      "0.0011634378461167216\n",
      "0.0011635813862085342\n",
      "0.0011637272546067834\n",
      "0.0011638669529929757\n",
      "0.001164008746854961\n",
      "0.0011641468154266477\n",
      "0.001164283836260438\n",
      "0.0011644158512353897\n",
      "0.0011645527556538582\n",
      "0.0011646797647699714\n",
      "0.0011648117797449231\n",
      "0.0011649399530142546\n",
      "0.0011650648666545749\n",
      "0.0011651808163151145\n",
      "0.0011653073597699404\n",
      "0.001165426685474813\n",
      "0.0011655554408207536\n",
      "0.0011656642891466618\n",
      "0.0011657786089926958\n",
      "0.0011658892035484314\n",
      "0.0011660035233944654\n",
      "0.0011661151656880975\n",
      "0.0011662262259051204\n",
      "0.0011663318146020174\n",
      "0.00116643903311342\n",
      "0.001166547299362719\n",
      "0.001166649628430605\n",
      "0.0011667468352243304\n",
      "0.001166847418062389\n",
      "0.0011669446248561144\n",
      "0.0011670475360006094\n",
      "0.001167139271274209\n",
      "0.001167232054285705\n",
      "0.0011673265835270286\n",
      "0.0011674215784296393\n",
      "0.001167513313703239\n",
      "0.0011676001595333219\n",
      "0.0011676891008391976\n",
      "0.0011677824659273028\n",
      "0.001167870475910604\n",
      "0.0011679521994665265\n",
      "0.0011680368334054947\n",
      "0.0011681197211146355\n",
      "0.0011681986507028341\n",
      "0.0011682827025651932\n",
      "0.0011683633783832192\n",
      "0.0011684446362778544\n",
      "0.0011685214703902602\n",
      "0.0011685981880873442\n",
      "0.0011686751386150718\n",
      "0.0011687524383887649\n",
      "0.001168829738162458\n",
      "0.0011689005186781287\n",
      "0.001168972346931696\n",
      "0.0011690447572618723\n",
      "0.0011691164691001177\n",
      "0.0011691806139424443\n",
      "0.0011692530242726207\n",
      "0.0011693231062963605\n",
      "0.001169386669062078\n",
      "0.0011694589629769325\n",
      "0.0011695213615894318\n",
      "0.0011695799184963107\n",
      "0.0011696405708789825\n",
      "0.0011697119334712625\n",
      "0.0011697771260514855\n",
      "0.0011698349844664335\n",
      "0.001169898547232151\n",
      "0.0011699569877237082\n",
      "0.0011700168251991272\n",
      "0.0011700757313519716\n",
      "0.0011701376643031836\n",
      "0.0011701956391334534\n",
      "0.0011702546617016196\n",
      "0.0011703136842697859\n",
      "0.0011703695636242628\n",
      "0.0011704162461683154\n",
      "0.0011704647913575172\n",
      "0.0011705277720466256\n",
      "0.0011705763172358274\n",
      "0.00117063382640481\n",
      "0.0011706850491464138\n",
      "0.0011707363883033395\n",
      "0.0011707843514159322\n",
      "0.0011708317324519157\n",
      "0.0011708824895322323\n",
      "0.001170929754152894\n",
      "0.0011709840036928654\n",
      "0.0011710290564224124\n",
      "0.0011710776016116142\n",
      "0.001171122188679874\n",
      "0.0011711602564901114\n",
      "0.001171214273199439\n",
      "0.0011712572304531932\n",
      "0.0011713085696101189\n",
      "0.0011713443091139197\n",
      "0.0011713922722265124\n",
      "0.0011714368592947721\n",
      "0.0011714770225808024\n",
      "0.0011715205619111657\n",
      "0.001171564101241529\n",
      "0.001171607756987214\n",
      "0.0011716528097167611\n",
      "0.0011716857552528381\n",
      "0.001171718118712306\n",
      "0.0011717637535184622\n",
      "0.0011718051973730326\n",
      "0.0011718437308445573\n",
      "0.0011718805180862546\n",
      "0.0011719150934368372\n",
      "0.0011719547910615802\n",
      "0.0011719904141500592\n",
      "0.0011720218462869525\n",
      "0.0011720730690285563\n",
      "0.0011721010087057948\n",
      "0.0011721373302862048\n",
      "0.0011721702758222818\n",
      "0.0011722014751285315\n",
      "0.0011722416384145617\n",
      "0.0011722735362127423\n",
      "0.001172312069684267\n",
      "0.001172344433143735\n",
      "0.0011723723728209734\n",
      "0.001172404270619154\n",
      "0.0011724433861672878\n",
      "0.0011724623618647456\n",
      "0.001172492397017777\n",
      "0.001172529300674796\n",
      "0.0011725600343197584\n",
      "0.0011725990334525704\n",
      "0.0011726253433153033\n",
      "0.0011726550292223692\n",
      "0.0011726856464520097\n",
      "0.001172720338217914\n",
      "0.0011727449018508196\n",
      "0.0011727616656571627\n",
      "0.0011728090466931462\n",
      "0.0011728275567293167\n",
      "0.0011728566605597734\n",
      "0.0011728761019185185\n",
      "0.0011729096295312047\n",
      "0.0011729319812729955\n",
      "0.0011729537509381771\n",
      "0.0011729890247806907\n",
      "0.0011730068363249302\n",
      "0.001173039199784398\n",
      "0.0011730738915503025\n",
      "0.0011730894912034273\n",
      "0.0011731140548363328\n",
      "0.0011731431586667895\n",
      "0.0011731610866263509\n",
      "0.0011731985723599792\n",
      "0.001173226977698505\n",
      "0.0011732337297871709\n",
      "0.0011732493294402957\n",
      "0.0011732772691175342\n",
      "0.001173299620859325\n",
      "0.001173319760710001\n",
      "0.0011733482824638486\n",
      "0.0011733768042176962\n",
      "0.0011733896099030972\n",
      "0.0011734169675037265\n",
      "0.0011734304716810584\n",
      "0.0011734639992937446\n",
      "0.00117348856292665\n",
      "0.0011734975269064307\n",
      "0.001173514174297452\n",
      "0.0011735516600310802\n",
      "0.0011735722655430436\n",
      "0.0011735818115994334\n",
      "0.001173603581264615\n",
      "0.0011736382730305195\n",
      "0.0011736523592844605\n",
      "0.0011736678425222635\n",
      "0.0011736858868971467\n",
      "0.0011737009044736624\n",
      "0.0011737322201952338\n",
      "0.0011737410677596927\n",
      "0.0011737735476344824\n",
      "0.0011737886816263199\n",
      "0.001173791359178722\n",
      "0.001173809403553605\n",
      "0.0011738355970010161\n",
      "0.001173865282908082\n",
      "0.001173855154775083\n",
      "0.001173884840682149\n",
      "0.001173895550891757\n",
      "0.001173914410173893\n",
      "0.0011739301262423396\n",
      "0.001173954689875245\n",
      "0.0011739680776372552\n",
      "0.0011739871697500348\n",
      "0.0011739927576854825\n",
      "0.001174000557512045\n",
      "0.0011740117333829403\n",
      "0.0011740397894755006\n",
      "0.0011740666814148426\n",
      "0.0011740581830963492\n",
      "0.0011740989284589887\n",
      "0.0011741162743419409\n",
      "0.0011741392081603408\n",
      "0.0011741291964426637\n",
      "0.0011741481721401215\n",
      "0.001174173434264958\n",
      "0.0011741913622245193\n",
      "0.001174196251668036\n",
      "0.0011741940397769213\n",
      "0.0011742085916921496\n",
      "0.0011742309434339404\n",
      "0.0011742353672161698\n",
      "0.0011742600472643971\n",
      "0.0011742879869416356\n",
      "0.0011742913629859686\n",
      "0.001174279605038464\n",
      "0.0011743047507479787\n",
      "0.0011743400245904922\n",
      "0.00117432267870754\n",
      "0.0011743423528969288\n",
      "0.00117438193410635\n",
      "0.0011743679642677307\n",
      "0.0011743769282475114\n",
      "0.0011744110379368067\n",
      "0.0011744032381102443\n",
      "0.0011744116200134158\n",
      "0.001174449105747044\n",
      "0.0011744501534849405\n",
      "0.0011744614457711577\n",
      "0.0011744658695533872\n",
      "0.001174464705400169\n",
      "0.0011744782095775008\n",
      "0.001174475415609777\n",
      "0.0011745129013434052\n",
      "0.0011745230294764042\n",
      "0.0011745240772143006\n",
      "0.0011745296651497483\n",
      "0.0011745570227503777\n",
      "0.0011745637748390436\n",
      "0.0011745844967663288\n",
      "0.0011745912488549948\n",
      "0.0011746035888791084\n",
      "0.0011745922965928912\n",
      "0.0011746236123144627\n",
      "0.0011746203526854515\n",
      "0.0011746236123144627\n",
      "0.0011746477102860808\n",
      "0.001174658304080367\n",
      "0.0011746756499633193\n",
      "0.0011746846139431\n",
      "0.0011746638920158148\n",
      "0.0011746918316930532\n",
      "0.0011746794916689396\n",
      "0.001174688572064042\n",
      "0.0011747266398742795\n",
      "0.0011747075477614999\n",
      "0.0011747254757210612\n",
      "0.0011747315293177962\n",
      "0.0011747388634830713\n",
      "0.0011747635435312986\n",
      "0.0011747388634830713\n",
      "0.0011747769312933087\n",
      "0.0011747769312933087\n",
      "0.0011747641256079078\n",
      "0.0011747825192287564\n",
      "0.0011748233810067177\n",
      "0.0011748261749744415\n",
      "0.00117483118083328\n",
      "0.0011748172109946609\n",
      "DONE\n",
      "Actual Length is tensor(11)\n",
      "0.10187552124261856\n",
      "0.09691096097230911\n",
      "0.09236244112253189\n",
      "0.08849737048149109\n",
      "0.08549300581216812\n",
      "0.0833401009440422\n",
      "0.08186271041631699\n",
      "0.08093427866697311\n",
      "0.08036461472511292\n",
      "0.08004473149776459\n",
      "0.07992499321699142\n",
      "0.0799606442451477\n",
      "0.0801236554980278\n",
      "0.08041057735681534\n",
      "0.08079907298088074\n",
      "0.08127354830503464\n",
      "0.08181683719158173\n",
      "0.08242054283618927\n",
      "0.08306695520877838\n",
      "0.0837288424372673\n",
      "0.08438818156719208\n",
      "0.08502355962991714\n",
      "0.08562120795249939\n",
      "0.08617591857910156\n",
      "0.08670210093259811\n",
      "0.08721226453781128\n",
      "0.08769915997982025\n",
      "0.08815724402666092\n",
      "0.08858295530080795\n",
      "0.08897554874420166\n",
      "0.08933688700199127\n",
      "0.08966702967882156\n",
      "0.08997014909982681\n",
      "0.09025009721517563\n",
      "0.09051103889942169\n",
      "0.0907571017742157\n",
      "0.09099020063877106\n",
      "0.09121336042881012\n",
      "0.09142837673425674\n",
      "0.09163688123226166\n",
      "0.091839499771595\n",
      "0.09203720092773438\n",
      "0.09223110973834991\n",
      "0.0924220010638237\n",
      "0.09261057525873184\n",
      "0.0927971675992012\n",
      "0.09298229217529297\n",
      "0.0931667909026146\n",
      "0.09334972500801086\n",
      "0.0935313031077385\n",
      "0.093710757791996\n",
      "0.09388899803161621\n",
      "0.09406577050685883\n",
      "0.0942409336566925\n",
      "0.09441375732421875\n",
      "0.09458465874195099\n",
      "0.0947532057762146\n",
      "0.09491926431655884\n",
      "0.09508325159549713\n",
      "0.09524542838335037\n",
      "0.095406174659729\n",
      "0.09556537121534348\n",
      "0.0957225114107132\n",
      "0.09587667882442474\n",
      "0.0960283875465393\n",
      "0.09617779403924942\n",
      "0.09632498770952225\n",
      "0.09647034853696823\n",
      "0.09661423414945602\n",
      "0.09675684571266174\n",
      "0.0968981385231018\n",
      "0.09703824669122696\n",
      "0.09717698395252228\n",
      "0.09731394797563553\n",
      "0.09744860976934433\n",
      "0.09758094698190689\n",
      "0.09771152585744858\n",
      "0.09784075617790222\n",
      "0.09796877950429916\n",
      "0.09809581935405731\n",
      "0.09822206944227219\n",
      "0.09834792464971542\n",
      "0.09847292304039001\n",
      "0.09859719127416611\n",
      "0.09872085601091385\n",
      "0.09884326159954071\n",
      "0.09896344691514969\n",
      "0.09908098727464676\n",
      "0.09919644147157669\n",
      "0.09931030869483948\n",
      "0.09942285716533661\n",
      "0.09953425824642181\n",
      "0.09964489936828613\n",
      "0.09975466132164001\n",
      "0.09986354410648346\n",
      "0.09997186809778214\n",
      "0.1000795066356659\n",
      "0.1001865491271019\n",
      "0.10029258579015732\n",
      "0.10039768368005753\n",
      "0.10050204396247864\n",
      "0.10060567408800125\n",
      "0.10070853680372238\n",
      "0.10081076622009277\n",
      "0.10091233998537064\n",
      "0.10101333260536194\n",
      "0.10111362487077713\n",
      "0.10121261328458786\n",
      "0.10131040960550308\n",
      "0.10140722244977951\n",
      "0.10150320082902908\n",
      "0.10159848630428314\n",
      "0.10169315338134766\n",
      "0.10178732872009277\n",
      "0.10188092291355133\n",
      "0.10197106748819351\n",
      "0.10205113142728806\n",
      "0.10212507843971252\n",
      "0.10219541192054749\n",
      "0.10226371139287949\n",
      "0.10233061760663986\n",
      "0.10239677131175995\n",
      "0.10246248543262482\n",
      "0.10252788662910461\n",
      "0.10259317606687546\n",
      "0.10265818983316422\n",
      "0.102723129093647\n",
      "0.10278808325529099\n",
      "0.10285284370183945\n",
      "0.10291747003793716\n",
      "0.10298199951648712\n",
      "0.10304637253284454\n",
      "0.10311071574687958\n",
      "0.10317493975162506\n",
      "0.10323894023895264\n",
      "0.10330282896757126\n",
      "0.10336657613515854\n",
      "0.10343018919229507\n",
      "0.10349360853433609\n",
      "0.10355686396360397\n",
      "0.10361988842487335\n",
      "0.103682741522789\n",
      "0.10374536365270615\n",
      "0.10380779206752777\n",
      "0.10387000441551208\n",
      "0.10393202304840088\n",
      "0.1039939597249031\n",
      "0.10405582934617996\n",
      "0.10411740839481354\n",
      "0.10417869687080383\n",
      "0.10423971712589264\n",
      "0.10430049896240234\n",
      "0.10436097532510757\n",
      "0.10442156344652176\n",
      "0.10448240488767624\n",
      "0.10454310476779938\n",
      "0.10460353642702103\n",
      "0.10466370731592178\n",
      "0.10472359508275986\n",
      "0.10478337109088898\n",
      "0.10484269261360168\n",
      "0.10490157455205917\n",
      "0.10495965927839279\n",
      "0.1050167828798294\n",
      "0.10507319867610931\n",
      "0.10512896627187729\n",
      "0.10518427193164825\n",
      "0.10523924231529236\n",
      "0.10529385507106781\n",
      "0.10534817725419998\n",
      "0.1054021418094635\n",
      "0.10545554012060165\n",
      "0.10550839453935623\n",
      "0.10556089878082275\n",
      "0.10561299324035645\n",
      "0.10566484928131104\n",
      "0.10571645945310593\n",
      "0.10576769709587097\n",
      "0.1058187335729599\n",
      "0.10586942732334137\n",
      "0.10591990500688553\n",
      "0.10597006231546402\n",
      "0.1060195118188858\n",
      "0.10606855154037476\n",
      "0.10611721873283386\n",
      "0.1061655580997467\n",
      "0.10621367394924164\n",
      "0.10626150667667389\n",
      "0.10630913078784943\n",
      "0.10635639727115631\n",
      "0.10640335083007812\n",
      "0.10645003616809845\n",
      "0.10649649798870087\n",
      "0.10654266923666\n",
      "0.10658863186836243\n",
      "0.10663436353206635\n",
      "0.10667991638183594\n",
      "0.10672523081302643\n",
      "0.10677023977041245\n",
      "0.10681536793708801\n",
      "0.10686024278402328\n",
      "0.10690503567457199\n",
      "0.10694947838783264\n",
      "0.10699370503425598\n",
      "0.10703763365745544\n",
      "0.10708127915859222\n",
      "0.10712472349405289\n",
      "0.10716801136732101\n",
      "0.10721093416213989\n",
      "0.10725323855876923\n",
      "0.10729499161243439\n",
      "0.10733634978532791\n",
      "0.10737744718790054\n",
      "0.10741826891899109\n",
      "0.10745883733034134\n",
      "0.10749873518943787\n",
      "0.10753807425498962\n",
      "0.10757697373628616\n",
      "0.10761567205190659\n",
      "0.10765385627746582\n",
      "0.10769177973270416\n",
      "0.10772935301065445\n",
      "0.1077667623758316\n",
      "0.10780400037765503\n",
      "0.10784098505973816\n",
      "0.10787783563137054\n",
      "0.1079145297408104\n",
      "0.1079510748386383\n",
      "0.1079874336719513\n",
      "0.10802364349365234\n",
      "0.10805972665548325\n",
      "0.10809561610221863\n",
      "0.10813143104314804\n",
      "0.10816703736782074\n",
      "0.1082025095820427\n",
      "0.10823782533407211\n",
      "0.10827304422855377\n",
      "0.10830805450677872\n",
      "0.10834302008152008\n",
      "0.10837775468826294\n",
      "0.10841234028339386\n",
      "0.10844684392213821\n",
      "0.10848115384578705\n",
      "0.10851535201072693\n",
      "0.10854937881231308\n",
      "0.1085832417011261\n",
      "0.10861696302890778\n",
      "0.1086505576968193\n",
      "0.10868406295776367\n",
      "0.10871739685535431\n",
      "0.1087505966424942\n",
      "0.10878362506628036\n",
      "0.10881645232439041\n",
      "0.10884911566972733\n",
      "0.1088816225528717\n",
      "0.10891397297382355\n",
      "0.10894619673490524\n",
      "0.1089782863855362\n",
      "0.10901030153036118\n",
      "0.10904211550951004\n",
      "0.10907390713691711\n",
      "0.10910549014806747\n",
      "0.1091369017958641\n",
      "0.10916827619075775\n",
      "0.10919982939958572\n",
      "0.10923174023628235\n",
      "0.1092633530497551\n",
      "0.10929472744464874\n",
      "0.10932592302560806\n",
      "0.10935701429843903\n",
      "0.10938791930675507\n",
      "0.10941875725984573\n",
      "0.1094493716955185\n",
      "0.10947996377944946\n",
      "0.1095103919506073\n",
      "0.1095406711101532\n",
      "0.10957090556621552\n",
      "0.10960116982460022\n",
      "0.10963144898414612\n",
      "0.10966166108846664\n",
      "0.10969171673059464\n",
      "0.10972168296575546\n",
      "0.10975147038698196\n",
      "0.10978122055530548\n",
      "0.10981079936027527\n",
      "0.1098402813076973\n",
      "0.10986972600221634\n",
      "0.10989899933338165\n",
      "0.10992807894945145\n",
      "0.10995715111494064\n",
      "0.10998611152172089\n",
      "0.11001496016979218\n",
      "0.11004365235567093\n",
      "0.1100722998380661\n",
      "0.11010075360536575\n",
      "0.11012917757034302\n",
      "0.11015747487545013\n",
      "0.11018569767475128\n",
      "0.1102137491106987\n",
      "0.11024172604084015\n",
      "0.11026961356401443\n",
      "0.11029742658138275\n",
      "0.11032511293888092\n",
      "0.11035268008708954\n",
      "0.11038020998239517\n",
      "0.11040755361318588\n",
      "0.1104348674416542\n",
      "0.1104620099067688\n",
      "0.1104905977845192\n",
      "0.11052016913890839\n",
      "0.11055023968219757\n",
      "0.11058060824871063\n",
      "0.1106109768152237\n",
      "0.11064145714044571\n",
      "0.11067178845405579\n",
      "0.11070208251476288\n",
      "0.110732302069664\n",
      "0.11076237261295319\n",
      "0.11079239100217819\n",
      "0.11082220822572708\n",
      "0.11085197329521179\n",
      "0.11088153719902039\n",
      "0.11091109365224838\n",
      "0.11094047874212265\n",
      "0.11096975952386856\n",
      "0.11099893599748611\n",
      "0.11102794110774994\n",
      "0.11105690896511078\n",
      "0.11108577996492386\n",
      "0.11111445724964142\n",
      "0.1111430823802948\n",
      "0.1111716628074646\n",
      "0.1112000048160553\n",
      "0.1112283393740654\n",
      "0.11125648021697998\n",
      "0.11128454655408859\n",
      "0.11131250113248825\n",
      "0.11134052276611328\n",
      "0.11136849969625473\n",
      "0.111396424472332\n",
      "0.11142425239086151\n",
      "0.11145206540822983\n",
      "0.11147971451282501\n",
      "0.11150724440813065\n",
      "0.11153475940227509\n",
      "0.11156216263771057\n",
      "0.11158940941095352\n",
      "0.11161665618419647\n",
      "0.1116437092423439\n",
      "0.11167067289352417\n",
      "0.11169761419296265\n",
      "0.11172439157962799\n",
      "0.11175108700990677\n",
      "0.11177773773670197\n",
      "0.11180425435304642\n",
      "0.1118307113647461\n",
      "0.11185700446367264\n",
      "0.1118832677602768\n",
      "0.11190938949584961\n",
      "0.11193545907735825\n",
      "0.11196143925189972\n",
      "0.11198735237121582\n",
      "0.11201301217079163\n",
      "0.11203860491514206\n",
      "0.11206413060426712\n",
      "0.11208955198526382\n",
      "0.11211486160755157\n",
      "0.11214006692171097\n",
      "0.11216516047716141\n",
      "0.11219032853841782\n",
      "0.11221548169851303\n",
      "0.11224056035280228\n",
      "0.1122656762599945\n",
      "0.1122906431555748\n",
      "0.11231550574302673\n",
      "0.11234044283628464\n",
      "0.11236517876386642\n",
      "0.11238987743854523\n",
      "0.11241443455219269\n",
      "0.11243896931409836\n",
      "0.11246340721845627\n",
      "0.11248765885829926\n",
      "0.11251179873943329\n",
      "0.11253582686185837\n",
      "0.11255977302789688\n",
      "0.11258365213871002\n",
      "0.11260747164487839\n",
      "0.11263119429349899\n",
      "0.11265483498573303\n",
      "0.11267823725938797\n",
      "0.11270160973072052\n",
      "0.11272482573986053\n",
      "0.112747922539711\n",
      "0.11277104914188385\n",
      "0.11279406398534775\n",
      "0.11281704902648926\n",
      "0.11283988505601883\n",
      "0.11286260932683945\n",
      "DONE\n",
      "Actual Length is tensor(7)\n",
      "0.023400843143463135\n",
      "0.03393149748444557\n",
      "0.03335992991924286\n",
      "0.03531689941883087\n",
      "0.03662582114338875\n",
      "0.03752860799431801\n",
      "0.038451001048088074\n",
      "0.03919387608766556\n",
      "0.03979527950286865\n",
      "0.040292467921972275\n",
      "0.04072766378521919\n",
      "0.041085001081228256\n",
      "0.04140444099903107\n",
      "0.041698623448610306\n",
      "0.04197138920426369\n",
      "0.0422343946993351\n",
      "0.042491137981414795\n",
      "0.04271632060408592\n",
      "0.0428963340818882\n",
      "0.04304042086005211\n",
      "0.043157052248716354\n",
      "0.04325266182422638\n",
      "0.043332457542419434\n",
      "0.043398696929216385\n",
      "0.04345439746975899\n",
      "0.04350144788622856\n",
      "0.043540358543395996\n",
      "0.04357347637414932\n",
      "0.043606534600257874\n",
      "0.04364083334803581\n",
      "0.04367581009864807\n",
      "0.043709203600883484\n",
      "0.04374021664261818\n",
      "0.043769754469394684\n",
      "0.04379777982831001\n",
      "0.043824452906847\n",
      "0.04384983330965042\n",
      "0.04387399181723595\n",
      "0.043897103518247604\n",
      "0.04391906037926674\n",
      "0.043939732015132904\n",
      "0.04395948722958565\n",
      "0.0439792275428772\n",
      "0.04399868845939636\n",
      "0.044017426669597626\n",
      "0.04403527453541756\n",
      "0.04405232518911362\n",
      "0.04407006502151489\n",
      "0.04408690333366394\n",
      "0.04410308599472046\n",
      "0.04411879554390907\n",
      "0.04413415119051933\n",
      "0.044149212539196014\n",
      "0.04416416585445404\n",
      "0.04417909309267998\n",
      "0.04419415444135666\n",
      "0.044209375977516174\n",
      "0.04422467201948166\n",
      "0.04423860087990761\n",
      "0.044251423329114914\n",
      "0.044263508170843124\n",
      "0.04427511617541313\n",
      "0.04428628832101822\n",
      "0.044297028332948685\n",
      "0.044307559728622437\n",
      "0.044318001717329025\n",
      "0.04432852566242218\n",
      "0.044339146465063095\n",
      "0.044350188225507736\n",
      "0.044361576437950134\n",
      "0.044373683631420135\n",
      "0.0443865992128849\n",
      "0.0443999208509922\n",
      "0.04441370069980621\n",
      "0.044427938759326935\n",
      "0.04444257915019989\n",
      "0.044457584619522095\n",
      "0.04447290301322937\n",
      "0.044488538056612015\n",
      "0.044504422694444656\n",
      "0.04452008008956909\n",
      "0.0445358008146286\n",
      "0.04455173388123512\n",
      "0.04456774890422821\n",
      "0.04458390548825264\n",
      "0.04460024833679199\n",
      "0.04461677372455597\n",
      "0.04463338106870651\n",
      "0.0446501150727272\n",
      "0.04466696083545685\n",
      "0.04468391090631485\n",
      "0.044700976461172104\n",
      "0.04471798613667488\n",
      "0.04473494738340378\n",
      "0.04475129768252373\n",
      "0.04476730152964592\n",
      "0.04478318616747856\n",
      "0.04479892551898956\n",
      "0.04481463134288788\n",
      "0.04483010619878769\n",
      "0.04484548792243004\n",
      "0.04486075043678284\n",
      "0.044875673949718475\n",
      "0.04488956928253174\n",
      "0.044902730733156204\n",
      "0.0449153296649456\n",
      "0.04492755979299545\n",
      "0.04493945091962814\n",
      "0.04495115578174591\n",
      "0.04496273398399353\n",
      "0.04497421532869339\n",
      "0.04498560354113579\n",
      "0.04499693587422371\n",
      "0.045008182525634766\n",
      "0.04501938819885254\n",
      "0.04503051936626434\n",
      "0.04504157975316048\n",
      "0.045052558183670044\n",
      "0.04506351053714752\n",
      "0.045074328780174255\n",
      "0.045084916055202484\n",
      "0.045095186680555344\n",
      "0.045105185359716415\n",
      "0.045114919543266296\n",
      "0.04512428864836693\n",
      "0.04513350501656532\n",
      "0.0451425164937973\n",
      "0.045151352882385254\n",
      "0.045160792768001556\n",
      "0.04516995698213577\n",
      "0.0451788492500782\n",
      "0.04518759250640869\n",
      "0.04519617557525635\n",
      "0.04520461708307266\n",
      "0.04521293565630913\n",
      "0.0452207587659359\n",
      "0.045227646827697754\n",
      "0.045234180986881256\n",
      "0.04524049535393715\n",
      "0.04524661600589752\n",
      "0.045252639800310135\n",
      "0.04525858163833618\n",
      "0.04526442289352417\n",
      "0.045270223170518875\n",
      "0.04527593404054642\n",
      "0.045281603932380676\n",
      "0.04528721794486046\n",
      "0.045292776077985764\n",
      "0.0452982634305954\n",
      "0.045303698629140854\n",
      "0.04530905932188034\n",
      "0.04531437158584595\n",
      "0.045319631695747375\n",
      "0.04532485082745552\n",
      "0.04533001035451889\n",
      "0.04533509537577629\n",
      "0.045340120792388916\n",
      "0.045345086604356766\n",
      "0.04534998908638954\n",
      "0.04535484313964844\n",
      "0.045359596610069275\n",
      "0.04536427557468414\n",
      "0.04536890238523483\n",
      "0.04537339508533478\n",
      "0.045377857983112335\n",
      "0.04538225010037422\n",
      "0.04538658633828163\n",
      "0.04539087414741516\n",
      "0.045395106077194214\n",
      "0.04539922997355461\n",
      "0.045403365045785904\n",
      "0.04540751874446869\n",
      "0.045412108302116394\n",
      "0.04541680961847305\n",
      "0.04542156681418419\n",
      "0.045426275581121445\n",
      "0.04543096572160721\n",
      "0.04543563723564148\n",
      "0.04544023796916008\n",
      "0.0454452745616436\n",
      "0.04545016586780548\n",
      "0.04545491561293602\n",
      "0.04545960947871208\n",
      "0.04546419531106949\n",
      "0.04546872526407242\n",
      "0.04547320678830147\n",
      "0.045477647334337234\n",
      "0.04548201337456703\n",
      "0.045486290007829666\n",
      "0.045490533113479614\n",
      "0.04549466818571091\n",
      "0.04549875855445862\n",
      "0.04550279304385185\n",
      "0.04550677165389061\n",
      "0.0455106757581234\n",
      "0.04551452398300171\n",
      "0.04551832750439644\n",
      "0.04552209749817848\n",
      "0.04552578926086426\n",
      "0.04552942141890526\n",
      "0.04553302004933357\n",
      "0.04553654417395592\n",
      "0.045540012419223785\n",
      "0.04554346576333046\n",
      "0.04554685950279236\n",
      "0.04555019363760948\n",
      "0.045553453266620636\n",
      "0.045556701719760895\n",
      "0.04555977135896683\n",
      "0.045562535524368286\n",
      "0.04556524381041527\n",
      "0.045567888766527176\n",
      "0.04557047784328461\n",
      "0.045573074370622635\n",
      "0.04557560384273529\n",
      "0.045578088611364365\n",
      "0.045580726116895676\n",
      "0.04558354616165161\n",
      "0.045586347579956055\n",
      "0.045589108020067215\n",
      "0.045591775327920914\n",
      "0.045594386756420135\n",
      "0.04559696093201637\n",
      "0.04559950530529022\n",
      "0.045601993799209595\n",
      "0.045604441314935684\n",
      "0.045606840401887894\n",
      "0.04560919478535652\n",
      "0.04561153054237366\n",
      "0.045613814145326614\n",
      "0.04561605677008629\n",
      "0.045618247240781784\n",
      "0.04562043398618698\n",
      "0.0456225648522377\n",
      "0.04562464356422424\n",
      "0.04562670737504959\n",
      "0.045628730207681656\n",
      "0.04563073813915253\n",
      "0.04563268646597862\n",
      "0.04563463106751442\n",
      "0.04563649743795395\n",
      "0.04563835635781288\n",
      "0.04564018175005913\n",
      "0.04564198479056358\n",
      "0.04564375430345535\n",
      "0.045645490288734436\n",
      "0.04564720392227173\n",
      "0.045648857951164246\n",
      "0.045650508254766464\n",
      "0.045652128756046295\n",
      "0.04565369710326195\n",
      "0.0456552729010582\n",
      "0.045656800270080566\n",
      "0.04565829783678055\n",
      "0.045659806579351425\n",
      "0.04566122591495514\n",
      "0.04566263034939766\n",
      "0.04566405341029167\n",
      "0.04566541314125061\n",
      "0.04566677287220955\n",
      "0.0456680990755558\n",
      "0.04566940292716026\n",
      "0.04567066207528114\n",
      "0.045671846717596054\n",
      "0.04567300155758858\n",
      "0.045674122869968414\n",
      "0.04567522555589676\n",
      "0.04567628353834152\n",
      "0.04567737504839897\n",
      "0.045678384602069855\n",
      "0.04567939043045044\n",
      "0.04568040370941162\n",
      "0.04568135738372803\n",
      "0.04568230360746384\n",
      "0.04568310081958771\n",
      "0.04568389430642128\n",
      "0.045684684067964554\n",
      "0.045685455203056335\n",
      "0.045686159282922745\n",
      "0.045686885714530945\n",
      "0.04568757489323616\n",
      "0.04568822681903839\n",
      "0.04568890109658241\n",
      "0.04568954557180405\n",
      "0.04569019004702568\n",
      "0.045690786093473434\n",
      "0.04569137841463089\n",
      "0.04569195955991745\n",
      "0.04569254070520401\n",
      "0.04569308087229729\n",
      "0.04569362476468086\n",
      "0.04569414258003235\n",
      "0.04569461941719055\n",
      "0.04569515958428383\n",
      "0.04569561779499054\n",
      "0.045696064829826355\n",
      "0.045696504414081573\n",
      "0.04569694772362709\n",
      "0.04569736868143082\n",
      "0.04569778963923454\n",
      "0.04569815471768379\n",
      "0.04569853097200394\n",
      "0.04569890350103378\n",
      "0.045699164271354675\n",
      "0.04569898918271065\n",
      "0.04569875821471214\n",
      "0.04569849744439125\n",
      "0.04569821432232857\n",
      "0.045697957277297974\n",
      "0.045697666704654694\n",
      "0.04569738730788231\n",
      "0.04569709673523903\n",
      "0.045696768909692764\n",
      "0.045696429908275604\n",
      "0.045696135610342026\n",
      "0.04569580778479576\n",
      "0.04569544270634651\n",
      "0.045695122331380844\n",
      "0.0456947460770607\n",
      "0.04569442570209503\n",
      "0.04569403827190399\n",
      "0.04569369554519653\n",
      "0.045693397521972656\n",
      "0.045693155378103256\n",
      "0.045692894607782364\n",
      "0.045692622661590576\n",
      "0.04569236561655998\n",
      "0.04569210484623909\n",
      "0.04569181427359581\n",
      "0.04569156467914581\n",
      "0.04569127410650253\n",
      "0.04569097235798836\n",
      "0.04569070041179657\n",
      "0.04569035768508911\n",
      "0.04569006711244583\n",
      "0.045689746737480164\n",
      "0.0456894226372242\n",
      "0.04568912461400032\n",
      "0.04568875953555107\n",
      "0.04568842425942421\n",
      "0.045688092708587646\n",
      "0.0456877201795578\n",
      "0.04568736255168915\n",
      "0.0456869900226593\n",
      "0.04568663612008095\n",
      "0.0456862598657608\n",
      "0.045685868710279465\n",
      "0.04568549245595932\n",
      "0.04568509757518768\n",
      "0.04568469151854515\n",
      "0.04568428546190262\n",
      "0.04568387195467949\n",
      "0.04568343609571457\n",
      "0.04568299651145935\n",
      "0.04568256437778473\n",
      "0.045682165771722794\n",
      "0.04568169265985489\n",
      "0.04568123444914818\n",
      "0.04568077623844147\n",
      "0.04568032920360565\n",
      "0.045679859817028046\n",
      "0.04567939043045044\n",
      "0.045678913593292236\n",
      "0.04567846655845642\n",
      "0.04567795991897583\n",
      "0.04567744955420494\n",
      "0.04567693918943405\n",
      "0.04567645117640495\n",
      "0.04567595198750496\n",
      "0.045675452798604965\n",
      "0.04567490890622139\n",
      "0.04567430913448334\n",
      "0.045673687011003494\n",
      "0.04567308351397514\n",
      "0.04567250981926918\n",
      "0.04567183554172516\n",
      "0.04567117616534233\n",
      "0.045670486986637115\n",
      "0.04566984251141548\n",
      "0.04566917568445206\n",
      "0.04566852003335953\n",
      "0.045667845755815506\n",
      "0.04566720128059387\n",
      "0.04566652700304985\n",
      "0.045665811747312546\n",
      "0.045665137469768524\n",
      "0.0456644706428051\n",
      "0.04566378518939018\n",
      "0.045663099735975266\n",
      "0.045662421733140945\n",
      "0.04566171392798424\n",
      "0.04566100984811783\n",
      "0.04566033557057381\n",
      "0.04565959423780441\n",
      "0.04565892368555069\n",
      "0.04565820470452309\n",
      "0.04565751925110817\n",
      "0.04565675929188728\n",
      "DONE\n",
      "Actual Length is tensor(14)\n",
      "0.0011712305713444948\n",
      "0.00118186604231596\n",
      "0.001158214290626347\n",
      "0.0011512701166793704\n",
      "0.0011501680128276348\n",
      "0.0011511610355228186\n",
      "0.0011528426548466086\n",
      "0.00115467538125813\n",
      "0.0011564393062144518\n",
      "0.0011580525897443295\n",
      "0.0011595014948397875\n",
      "0.0011607827618718147\n",
      "0.0011619155993685126\n",
      "0.0011629038490355015\n",
      "0.0011637782445177436\n",
      "0.0011645378544926643\n",
      "0.0011652152752503753\n",
      "0.0011658109724521637\n",
      "0.0011663350742310286\n",
      "0.0011667996877804399\n",
      "0.0011672098189592361\n",
      "0.0011675739660859108\n",
      "0.001167896669358015\n",
      "0.0011681803734973073\n",
      "0.0011684407945722342\n",
      "0.0011686660582199693\n",
      "0.001168865361250937\n",
      "0.0011690508108586073\n",
      "0.0011692139087244868\n",
      "0.0011693565174937248\n",
      "0.0011694823624566197\n",
      "0.0011695927241817117\n",
      "0.0011696997098624706\n",
      "0.0011697860900312662\n",
      "0.001169868977740407\n",
      "0.0011699364986270666\n",
      "0.0011700027389451861\n",
      "0.0011700596660375595\n",
      "0.001170107047073543\n",
      "0.00117015209980309\n",
      "0.0011701888870447874\n",
      "0.001170220086351037\n",
      "0.0011702446499839425\n",
      "0.001170269213616848\n",
      "0.0011702908668667078\n",
      "0.0011703070485964417\n",
      "0.0011703198542818427\n",
      "0.001170328352600336\n",
      "0.0011703382479026914\n",
      "0.00117034325376153\n",
      "0.0011703416239470243\n",
      "0.0011703483760356903\n",
      "0.001170347211882472\n",
      "0.0011703427881002426\n",
      "0.0011703427881002426\n",
      "0.0011703394120559096\n",
      "0.0011703271884471178\n",
      "0.0011703171767294407\n",
      "0.0011703165946528316\n",
      "0.0011703009949997067\n",
      "0.00117029482498765\n",
      "0.0011702837655320764\n",
      "0.0011702680494636297\n",
      "0.0011702579213306308\n",
      "0.001170243020169437\n",
      "0.0011702290503308177\n",
      "0.0011702222982421517\n",
      "0.0011701988987624645\n",
      "0.0011701894691213965\n",
      "0.0011701766634359956\n",
      "0.001170155475847423\n",
      "0.0011701398761942983\n",
      "0.001170119270682335\n",
      "0.0011701053008437157\n",
      "0.0011700920294970274\n",
      "0.0011700752656906843\n",
      "0.0011700596660375595\n",
      "0.0011700390605255961\n",
      "0.001170025672763586\n",
      "0.0011700061149895191\n",
      "0.0011699843453243375\n",
      "0.00116997049190104\n",
      "0.0011699519818648696\n",
      "0.0011699381284415722\n",
      "0.0011699197348207235\n",
      "0.0011699058813974261\n",
      "0.0011698841117322445\n",
      "0.0011698686284944415\n",
      "0.0011698523303493857\n",
      "0.0011698295129463077\n",
      "0.0011698172893375158\n",
      "0.0011698011076077819\n",
      "0.0011697810841724277\n",
      "0.0011697609443217516\n",
      "0.0011697475565597415\n",
      "0.0011697342852130532\n",
      "0.0011697141453623772\n",
      "0.0011697025038301945\n",
      "0.0011696829460561275\n",
      "0.0011696635046973825\n",
      "0.0011696495348587632\n",
      "0.0011696323053911328\n",
      "0.0011696100700646639\n",
      "0.0011696027358993888\n",
      "0.0011695805005729198\n",
      "0.0011695637367665768\n",
      "0.001169548137113452\n",
      "0.0011695320717990398\n",
      "0.0011695231078192592\n",
      "0.0011695045977830887\n",
      "0.0011694857385009527\n",
      "0.0011694657150655985\n",
      "0.0011694624554365873\n",
      "0.0011694439454004169\n",
      "0.0011694300919771194\n",
      "0.0011694167042151093\n",
      "0.0011694066924974322\n",
      "0.0011693788692355156\n",
      "0.001169373164884746\n",
      "0.0011693565174937248\n",
      "0.0011693432461470366\n",
      "0.0011693231062963605\n",
      "0.0011693042470142245\n",
      "0.001169308670796454\n",
      "0.0011692818952724338\n",
      "0.0011692752595990896\n",
      "0.0011692546540871263\n",
      "0.0011692384723573923\n",
      "0.0011692296247929335\n",
      "0.0011692083207890391\n",
      "0.0011692038970068097\n",
      "0.0011691960971802473\n",
      "0.001169176190160215\n",
      "0.0011691671097651124\n",
      "0.0011691482504829764\n",
      "0.0011691382387652993\n",
      "0.001169127062894404\n",
      "0.001169120310805738\n",
      "0.001169108203612268\n",
      "0.001169092021882534\n",
      "0.001169079216197133\n",
      "0.001169069204479456\n",
      "0.0011690596584230661\n",
      "0.0011690430110320449\n",
      "0.0011690319515764713\n",
      "0.00116901402361691\n",
      "0.001169007970020175\n",
      "0.001168995164334774\n",
      "0.001168979099020362\n",
      "0.0011689822422340512\n",
      "0.0011689623352140188\n",
      "0.0011689589591696858\n",
      "0.0011689243838191032\n",
      "0.001168929971754551\n",
      "0.001168917748145759\n",
      "0.0011689133243635297\n",
      "0.0011689065722748637\n",
      "0.0011688927188515663\n",
      "0.00116886873729527\n",
      "0.001168863265775144\n",
      "0.0011688520899042487\n",
      "0.0011688603553920984\n",
      "0.0011688309023156762\n",
      "0.0011688288068398833\n",
      "0.001168807502835989\n",
      "0.001168814254924655\n",
      "0.0011687974911183119\n",
      "0.0011687896912917495\n",
      "0.0011687662918120623\n",
      "0.0011687640799209476\n",
      "0.0011687518563121557\n",
      "0.0011687474325299263\n",
      "0.0011687480146065354\n",
      "0.0011687345104292035\n",
      "0.001168721355497837\n",
      "0.0011687073856592178\n",
      "0.0011687017977237701\n",
      "0.0011686906218528748\n",
      "0.0011686934158205986\n",
      "0.0011686728103086352\n",
      "0.0011686678044497967\n",
      "0.0011686549987643957\n",
      "0.0011686539510264993\n",
      "0.0011686417274177074\n",
      "0.001168630551546812\n",
      "0.0011686327634379268\n",
      "0.001168621121905744\n",
      "0.0011685993522405624\n",
      "0.0011685987701639533\n",
      "0.0011685803765431046\n",
      "0.0011685837525874376\n",
      "0.001168575370684266\n",
      "0.0011685671051964164\n",
      "0.0011685637291520834\n",
      "0.0011685414938256145\n",
      "0.001168542541563511\n",
      "0.0011685409117490053\n",
      "0.0011685281060636044\n",
      "0.0011685325298458338\n",
      "0.0011685180943459272\n",
      "0.0011685080826282501\n",
      "0.0011685008648782969\n",
      "0.0011684974888339639\n",
      "0.0011684936471283436\n",
      "0.0011684880591928959\n",
      "0.0011684696655720472\n",
      "0.001168475835584104\n",
      "0.0011684580240398645\n",
      "0.0011684562778100371\n",
      "0.0011684390483424067\n",
      "0.0011684352066367865\n",
      "0.0011684246128425002\n",
      "0.0011684224009513855\n",
      "0.0011684135533869267\n",
      "0.0011684157652780414\n",
      "0.001168407965451479\n",
      "0.0011683867778629065\n",
      "0.0011683923657983541\n",
      "0.0011683733901008964\n",
      "0.001168375601992011\n",
      "0.0011683623306453228\n",
      "0.0011683633783832192\n",
      "0.0011683651246130466\n",
      "0.001168350107036531\n",
      "0.0011683243792504072\n",
      "0.0011683455668389797\n",
      "0.0011683473130688071\n",
      "0.0011683200718834996\n",
      "0.0011683243792504072\n",
      "0.0011683121556416154\n",
      "0.0011683071497827768\n",
      "0.00116828887257725\n",
      "0.0011683021439239383\n",
      "0.0011682944605126977\n",
      "0.0011682843323796988\n",
      "0.0011682898038998246\n",
      "0.0011682772310450673\n",
      "0.0011682565091177821\n",
      "0.0011682737385854125\n",
      "0.0011682376498356462\n",
      "0.0011682548793032765\n",
      "0.0011682576732710004\n",
      "0.0011682542972266674\n",
      "0.0011682442855089903\n",
      "0.001168231014162302\n",
      "0.0011682320619001985\n",
      "0.0011682165786623955\n",
      "0.0011682159965857863\n",
      "0.0011682220501825213\n",
      "0.0011682031909003854\n",
      "0.0011682076146826148\n",
      "0.0011681908508762717\n",
      "0.001168182585388422\n",
      "0.0011681808391585946\n",
      "0.0011681714095175266\n",
      "0.0011681764153763652\n",
      "0.0011681659379974008\n",
      "0.001168169779703021\n",
      "0.001168153714388609\n",
      "0.0011681553442031145\n",
      "0.0011681581381708384\n",
      "0.0011681519681587815\n",
      "0.0011681591859087348\n",
      "0.0011681419564411044\n",
      "0.0011681197211146355\n",
      "0.0011681357864290476\n",
      "0.0011681107571348548\n",
      "0.0011681329924613237\n",
      "0.0011681219330057502\n",
      "0.0011681074975058436\n",
      "0.0011680746683850884\n",
      "0.001168101909570396\n",
      "0.0011680969037115574\n",
      "0.001168102491647005\n",
      "0.001168098533526063\n",
      "0.0011681097093969584\n",
      "0.0011680684983730316\n",
      "0.0011680740863084793\n",
      "0.0011680590687319636\n",
      "0.0011680640745908022\n",
      "0.0011680779280140996\n",
      "0.0011680752504616976\n",
      "0.0011680852621793747\n",
      "0.0011680428870022297\n",
      "0.0011680450988933444\n",
      "0.001168056856840849\n",
      "0.0011680456809699535\n",
      "0.0011680484749376774\n",
      "0.0011680196039378643\n",
      "0.0011680374154821038\n",
      "0.0011680306633934379\n",
      "0.0011680228635668755\n",
      "0.001168025191873312\n",
      "0.0011679994640871882\n",
      "0.001168011804111302\n",
      "0.0011680028401315212\n",
      "0.0011679884046316147\n",
      "0.0011679994640871882\n",
      "0.001168011804111302\n",
      "0.0011679945746436715\n",
      "0.0011680010939016938\n",
      "0.0011679672170430422\n",
      "0.0011679750168696046\n",
      "0.0011679844465106726\n",
      "0.0011679739691317081\n",
      "0.001167976763099432\n",
      "0.0011679617455229163\n",
      "0.0011679349699988961\n",
      "0.001167963957414031\n",
      "0.0011679460294544697\n",
      "0.0011679378803819418\n",
      "0.001167957321740687\n",
      "0.001167947193607688\n",
      "0.0011679495219141245\n",
      "0.0011679367162287235\n",
      "0.0011679243762046099\n",
      "0.0011679267045110464\n",
      "0.0011679255403578281\n",
      "0.0011679271701723337\n",
      "0.0011679105227813125\n",
      "0.001167899346910417\n",
      "0.0011679149465635419\n",
      "0.0011679071467369795\n",
      "0.00116790272295475\n",
      "0.0011679071467369795\n",
      "0.0011679149465635419\n",
      "0.0011679043527692556\n",
      "0.0011678938753902912\n",
      "0.0011678860755637288\n",
      "0.00116790272295475\n",
      "0.0011678826995193958\n",
      "0.0011678760638460517\n",
      "0.0011678849114105105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-26e4bf027099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9855925465e7>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[0;34m(X, Y, masks, n_seed, n_samples, max_length)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Loop until 400 timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mY_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mY_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mone_new_timestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-43d26281a090>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, padding_mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rdata/yelnady/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    320\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[1;32m    321\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "real_train_X0 ,real_train_Y_labels0 ,padding_mask0= get_one_class(real_train_X_cat ,real_train_Y_labels ,real_train_masks,0)\n",
    "real_train_X1 ,real_train_Y_labels1 ,padding_mask1= get_one_class(real_train_X_cat ,real_train_Y_labels ,real_train_masks,1)\n",
    "real_train_X2 ,real_train_Y_labels2 ,padding_mask2= get_one_class(real_train_X_cat ,real_train_Y_labels ,real_train_masks,2)\n",
    "real_train_X3 ,real_train_Y_labels3 ,padding_mask3= get_one_class(real_train_X_cat ,real_train_Y_labels ,real_train_masks,3)\n",
    "\n",
    "\n",
    "generate_dataset(real_train_X0 ,real_train_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_train_X0.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X1 ,real_train_Y_labels1 ,padding_mask1,n_seed=n_seed,n_samples=real_train_X1.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X2 ,real_train_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_train_X2.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X3 ,real_train_Y_labels3 ,padding_mask3,n_seed=n_seed,n_samples=real_train_X3.size(0),max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c81528-2a26-43a9-92c7-a13b65684ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to make sure everything is calculated correctly in softmax and argmax\n",
    "\n",
    "# x = torch.rand((2,3))\n",
    "# print(x)\n",
    "# print(F.softmax(x).argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ab8ea-f7f2-4c77-84e8-965c3a281831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# real_test_X0 ,real_test_Y_labels0 ,padding_mask0= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,0)\n",
    "# real_test_X1 ,real_test_Y_labels1 ,padding_mask1= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,1)\n",
    "# real_test_X2 ,real_test_Y_labels2 ,padding_mask2= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,2)\n",
    "# real_test_X3 ,real_test_Y_labels3 ,padding_mask3= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,3)\n",
    "\n",
    "\n",
    "# generate_dataset(real_test_X0 ,real_test_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_test_X0.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X1 ,real_test_Y_labels1 ,padding_mask1,n_seed=n_seed,n_samples=real_test_X1.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X2 ,real_test_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_test_X2.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X3 ,real_test_Y_labels3 ,padding_mask3,n_seed=n_seed,n_samples=real_test_X3.size(0),max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193017c-1930-4e3e-bdaa-29a9fcb36697",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('npz_transformer_token_V10.5',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474e225-9e29-41a4-a74c-36e2d292d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c638b2-11d3-4bd1-a128-1a225e530077",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect(),torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d6dec-bdbb-4961-956c-28eb2390b470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What is the average values of all samples in variable length sequence\n",
    "\n",
    "mean_of_samples = []\n",
    "for x,length in zip(real_train_X, real_train_lengths):\n",
    "    mean_of_samples.append(torch.sum(x)/(9*length)) # torch.Size([2500])\n",
    "    \n",
    "print(np.mean(mean_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e2e56-6e32-4d9c-b04f-d0eb8356742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(input) → Tensor\n",
    "# Returns the maximum value of all elements in the input tensor.\n",
    "\n",
    "torch.max(real_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71072b7-4b93-41d4-9894-c81bd6686485",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(real_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0674c9-e124-4dd3-99be-e7dc8ca16a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(real_train_X.flatten(), 'o', color='black');\n",
    "plt.xscale(\"log\")\n",
    "ax.set_title('CDF - Class all')\n",
    "ax.set_xlabel('The Sequence Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8cf9d-b707-430d-bf46-1f13e24d8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(real_train_X,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134539e-d658-4fa6-87f0-f75d99814867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(torch.mean(real_train_X,2).flatten(), 'o', color='black');\n",
    "plt.xscale(\"log\")\n",
    "ax.set_title('CDF - Class all')\n",
    "ax.set_xlabel('The Sequence Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6aa925-a487-46f4-b536-4cb11877f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([1,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d43f4-af58-47d6-abdb-bdb7c4e3c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
