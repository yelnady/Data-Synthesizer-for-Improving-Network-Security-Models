cuda:1
total    : 50962169856
free     : 24650383360
used     : 26311786496


generator_token_V10.3.py:280: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  F.softmax(x.squeeze()).argmax(2)[:,:40]
generator_token_V10.3.py:318: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  one_new_nlp = F.softmax(nlp_predicted.squeeze()).argmax(1)[-1].item()
0/3125
100/3125
200/3125
300/3125
400/3125
500/3125
600/3125
700/3125
800/3125
900/3125
1000/3125
1100/3125
1200/3125
1300/3125
1400/3125
1500/3125
1600/3125
1700/3125
1800/3125
1900/3125
2000/3125
2100/3125
2200/3125
2300/3125
2400/3125
2500/3125
2600/3125
2700/3125
2800/3125
2900/3125
3000/3125
3100/3125
0/8062
100/8062
200/8062
300/8062
400/8062
500/8062
600/8062
700/8062
800/8062
900/8062
1000/8062
1100/8062
1200/8062
1300/8062
1400/8062
1500/8062
1600/8062
1700/8062
1800/8062
1900/8062
2000/8062
2100/8062
2200/8062
2300/8062
2400/8062
2500/8062
2600/8062
2700/8062
2800/8062
2900/8062
3000/8062
3100/8062
3200/8062
3300/8062
3400/8062
3500/8062
3600/8062
3700/8062
3800/8062
3900/8062
4000/8062
4100/8062
4200/8062
4300/8062
4400/8062
4500/8062
4600/8062
4700/8062
4800/8062
4900/8062
5000/8062
5100/8062
5200/8062
5300/8062
5400/8062
5500/8062
5600/8062
5700/8062
5800/8062
5900/8062
6000/8062
6100/8062
Traceback (most recent call last):
  File "generator_token_V10.3.py", line 363, in <module>
    generate_dataset(real_train_X1 ,real_train_Y_labels1 , real_train_X_nlp_1, padding_mask1,n_seed=n_seed,n_samples=real_train_X1.size(0),max_length=max_length)
  File "generator_token_V10.3.py", line 315, in generate_dataset
    nlp_predicted = model_nlp(datapoint.to(device),src_mask.to(device),mask.to(device)).cpu()
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "generator_token_V10.3.py", line 254, in forward
    output = self.transformer_encoder(src, src_mask,padding_mask)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 195, in forward
    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/transformer.py", line 320, in forward
    src2 = self.self_attn(src, src, src, attn_mask=src_mask,
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1031, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 5082, in multi_head_attention_forward
    attn_output, attn_output_weights = _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)
  File "/rhome/yelnady/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 4823, in _scaled_dot_product_attention
    q = q / math.sqrt(E)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 1; 47.46 GiB total capacity; 12.87 GiB already allocated; 5.44 MiB free; 13.92 GiB reserved in total by PyTorch)
