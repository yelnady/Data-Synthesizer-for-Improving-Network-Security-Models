{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd804dce",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fde077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math, pickle\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "import seaborn as sns\n",
    "from tensorboard import default\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "sys.path.append('DG/gan')\n",
    "import gc\n",
    "print(device)\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fe389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 25396838400\n",
      "free     : 25385435136\n",
      "used     : 11403264\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafd34f",
   "metadata": {},
   "source": [
    "# Import Real Training Data to Generate New Data from it.\n",
    "\n",
    "### Actual Distribution\n",
    "- Class0: 6250\n",
    "- Class1: 16124\n",
    "- Class2: 21273\n",
    "- Class3: 5278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9cee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_class(X,Y,flag,class_label): # (X, Y, and flag) are the whole dataset that is consisted of many classes, Y is NOT One-Hot Encoded\n",
    "    indices_class_label = np.where(Y==class_label)\n",
    "    X,Y,flag = X[indices_class_label], Y[indices_class_label], flag[indices_class_label] \n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]\n",
    "\n",
    "def get_n_samples(X,Y,flag,n_samples):\n",
    "    randomList = random.sample(range(0, Y.shape[0]), n_samples)\n",
    "    return X[randomList], Y[randomList], flag[randomList]\n",
    "\n",
    "# In real data, if flag sum is 1 --> Then no timestep at all. --> So we do remove those ones by converting them to zeros, then remove from the list\n",
    "# In real data, there is no flag of length ZERO\n",
    "def remove_zero_datapoints(X,Y,flag):\n",
    "    indices_non_zero = torch.nonzero(torch.sum(flag,1)-1).squeeze()\n",
    "    return X[indices_non_zero], Y[indices_non_zero], flag[indices_non_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98039037",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_real = np.load('../data/google/data_train_reduced.npz')\n",
    "\n",
    "real_train_X = torch.from_numpy(training_real['data_feature']).float() #[50000, 2500, 9]\n",
    "real_train_Y = torch.from_numpy(training_real['data_attribute']) #[50000,4]\n",
    "real_train_Y_labels = torch.argmax(real_train_Y,1) #[50000,]  returns a list of the class label, no one hot encoding any more\n",
    "real_train_flags = torch.from_numpy(training_real['data_gen_flag'])   # (50000, 2500)\n",
    "\n",
    "#------------------------------------------------------------------Loading One Class------------------------------------------------\n",
    "real_train_X, real_train_Y_labels, real_train_flags= remove_zero_datapoints(real_train_X, real_train_Y_labels, real_train_flags)\n",
    "\n",
    "# The pading mask need to be inverted \n",
    "\n",
    "real_train_masks = real_train_flags == 0 # True when padding, False when considering\n",
    "\n",
    "real_train_lengths = torch.sum(real_train_flags,1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfce9df-46e6-468b-ae34-2926d40a7bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24462, 2500])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_train_X_nlp = (real_train_X.sum(2)*10**2).int()\n",
    "real_train_X_nlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7678a654-bebf-4fc1-b888-6d5593349af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "real_train_X_nlp = le.fit_transform(real_train_X_nlp.flatten())\n",
    "real_train_X_nlp = torch.Tensor(real_train_X_nlp).view(-1, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b111a884-cf51-4686-b7bd-31bd21c55dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(225.)\n"
     ]
    }
   ],
   "source": [
    "n_classes_disc = real_train_X_nlp.max() + 3 # The 3 are for padding and for end token and for starting from zero\n",
    "# IF 245 + 3 = 248, then put 246 as end token, and 247 as padding\n",
    "for i,after_last in enumerate(real_train_lengths):\n",
    "    real_train_X_nlp[i,after_last]  = n_classes_disc - 2  \n",
    "    real_train_X_nlp[i,after_last+1:] = torch.full((2500 - after_last - 1,), n_classes_disc - 1)\n",
    "\n",
    "print(n_classes_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4675258e-1194-427d-b4d1-f3c219f57ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes_disc = int(n_classes_disc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de89e125-f4d2-40ff-84d4-5670f3b17f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes_disc\n",
    "\n",
    "real_train_X_nlp = real_train_X_nlp.long()\n",
    "real_train_X_nlp = real_train_X_nlp[:,:400]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbdbd2e-e425-4471-9ef0-41d2802d62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe) # ,persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee02bed",
   "metadata": {},
   "source": [
    "# PyTorch NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678056d6-ba86-42f7-9619-c2382a31ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformerNLP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_features=9, d_model=256, n_heads=8, n_hidden=256, n_layers=8, dropout=0.0, S=400):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Time Series Transformer Model (Working Inshallah)'\n",
    "        self.InputLinear = nn.Embedding(n_classes_disc, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding2(d_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_classes_disc) #For each timestep, create a probability over 300 classes\n",
    "        self.init_weights()\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e6)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask,padding_mask):\n",
    "        src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask,padding_mask)\n",
    "        output = self.OutputLinear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b177a7-2637-4077-9932-beb292ec5502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nlp = TimeSeriesTransformerNLP().to(device)\n",
    "# ck = torch.load('lightning_logs/version_127/checkpoints/epoch=299-step=57599.ckpt')['state_dict']\n",
    "# model_classifier.load_state_dict(ck)\n",
    "model_nlp.load_state_dict(torch.load('W_transformer_token_V11.1'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a15ab68-e81a-4105-a594-77c338f3fcfc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   0,   7,  14,  19,  10,  10,  10,  10,  10,  10,  10,  10,\n",
      "         10,  10,  10,  10,  10,  10,   9,  10,  10,  10,  10,  11,  10,  15,\n",
      "         13,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   3,   3,   9,  11,  14,  19,  10,  10,  10,  10,  10,  10,  10,\n",
      "         10,  10,  10,  10,  10,  10,  10,   9,  10,  10,  10,  10,  11,  10,\n",
      "         15,  13,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,   0, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   5,   7, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,  30,  14,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,  14,  15,  15,  14,  23,  20,   4, 223, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([ 11,  14,  15,  15,  14,  21,  23,  20,   4, 223, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   6, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([ 14,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([ 14,   6,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([ 44,  30,  29,  30,  30,  29,  30,  29,  30,  30,  30,  29,  30,  29,\n",
      "         30,  29,  29,  28,  29,  27,  27,  26, 223, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([ 33,  29,  30,  29,  30,  30,  29,  30,  29,  30,  30,  30,  29,  30,\n",
      "         29,  30,  29,  29,  28,  29,  27,  27,  26, 223, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,   2,   2,   2,   2, 223, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   3,   2,   2,   2,   2,   0, 223, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,   0,  29, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   0,  21,  30,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,  33,  19,  20,  21,  21,  22,  29,  31,   1,   0, 223, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,  27,  38,  32,  19,  20,  21,  21,  22,  29,  31,   1,   0, 223,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   0,  32,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,   0,   0,  29,  72,  62,  66,  70,  61,  14,   1, 223, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   0,   0,   3,  11,  29,  72,  62,  66,  70,  61,  14,   1, 223,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,  24, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  4,  36,  24, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,  14,  15,  16,  16,  17,  15,   4, 223, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([ 10,  15,  15,  17,  16,  16,  17,  15,   4, 223, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,   3,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   0,   0,  16,   4, 223, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  0,  14,  16,  17,  16,   4, 223, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,  13,  15,  15,  15,  18,  15,   3, 223, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([ 11,  13,  14,  16,  16,  15,  18,  15,   3, 223, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  4,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  3,   0, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n",
      "tensor([  0,   3, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224],\n",
      "       device='cuda:1')\n",
      "tensor([  4,   3, 223, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_mask = model_nlp.generate_square_subsequent_mask(400)\n",
    "for i in range(2000,2020):\n",
    "    nlp_predicted = model_nlp(real_train_X_nlp[i:i+1].to(device),src_mask.to(device),None)\n",
    "    print(F.softmax(nlp_predicted[0],1)[:40].argmax(1))\n",
    "    print(real_train_X_nlp[i,:40])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8099a-08bc-4da3-a2e6-b15bd30a8c6a",
   "metadata": {},
   "source": [
    "# PyTorch Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfac6a17-632c-418c-8e53-41841807d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_features=9, d_model=256, n_heads=8, n_hidden=256, n_layers=8, dropout=0.0,S=400):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Time Series Transformer Model'\n",
    "        self.InputLinear = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding2(d_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_heads, n_hidden, dropout, batch_first = True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.OutputLinear = nn.Linear(d_model, n_features) # The output of the encoder is similar to the input of the encoder, both are (B,S,d_model)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float(-1e7)).masked_fill(mask == 1, float(0.0))\n",
    "        return mask \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.InputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.OutputLinear.bias.data.zero_()\n",
    "        self.OutputLinear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask,padding_mask):\n",
    "        src = self.InputLinear(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask,padding_mask)\n",
    "        output = self.OutputLinear(output)\n",
    "        output = self.activation(output) # output[...,:9] --> Actual 9 values\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43bb6f09-e1f9-40df-b72a-b2e7f181ad00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TimeSeriesTransformer().to(device)\n",
    "# ck = torch.load('lightning_logs/version_127/checkpoints/epoch=299-step=57599.ckpt')['state_dict']\n",
    "# model_classifier.load_state_dict(ck)\n",
    "model.load_state_dict(torch.load('../W_transformer_allV3_new'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "456d1b20-3158-4828-bcb9-7ce75883144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache(), gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d0c",
   "metadata": {},
   "source": [
    "# Generating New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91cd32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# The following is the generating part #################################\n",
    "\n",
    "# Returns: X (The data)\n",
    "# Returns: masks (e.g. [False,Flase,True,True,True,....,True]), False is the actual Data\n",
    "\n",
    "resulted_masks = []\n",
    "generated_dataset_Y=[]\n",
    "generated_dataset_X=[]\n",
    "\n",
    "def generate_dataset(X,Y,masks,n_seed,n_samples,max_length):\n",
    "    for n in range(n_samples):\n",
    "        \n",
    "        datapoint,y,mask = get_n_samples(X,Y,masks,n_samples=1) # The first 10 timesteps of just one sample\n",
    "        datapoint = datapoint[:,:n_seed] \n",
    "        datapoint_len = torch.sum(~mask) #Flip and count, you will get the actual length to generate likewise\n",
    "        mask = mask[:,:n_seed] \n",
    "        S = datapoint.size(1)\n",
    "        print('Actual Length is',datapoint_len)\n",
    "        for t in range(max_length-n_seed): # Loop until 400 timesteps\n",
    "            src_mask = model.generate_square_subsequent_mask(S)\n",
    "            \n",
    "            predicted = model(datapoint.to(device),src_mask.to(device),mask.to(device)).cpu()\n",
    "            one_new_timestep= predicted[:,-1].unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            datapoint_nlp =  (predicted.sum(2)*10**2).int()\n",
    "            datapoint_nlp = le.transform(datapoint_nlp.flatten())\n",
    "            datapoint_nlp = torch.Tensor(datapoint_nlp).view(-1, S)\n",
    "            nlp_predicted =  model_nlp(datapoint_nlp.long().to(device),src_mask.to(device),None).cpu()\n",
    "            probs = F.softmax(nlp_predicted.squeeze()) #Convert to probabilities\n",
    "            v,i = nlp_predicted.topk(1)   # Choose from the top k , returns values (probs) and indices (labels) \n",
    "            decider = random.choice(i[0,-1]) #Choose from the last timestep\n",
    "            \n",
    "            if decider == n_classes_disc-2 or decider == n_classes_disc-1:\n",
    "                print(S, datapoint_len )\n",
    "                datapoint = torch.cat((datapoint,torch.zeros((1,max_length-S,9))),1).cpu()# Pad remainings with zero\n",
    "                mask =  torch.cat((mask,torch.full((1,max_length-S),True)),1)\n",
    "                break\n",
    "            \n",
    "            datapoint = torch.cat((datapoint,one_new_timestep),1) # add the forecasted timestep\n",
    "            mask = torch.cat((mask,torch.tensor([[False]])),1 )\n",
    "            S = datapoint.size(1)\n",
    "        print('Done')\n",
    "        resulted_masks.append(mask.numpy())\n",
    "        generated_dataset_X.append(datapoint.squeeze().detach().numpy())\n",
    "        generated_dataset_Y.append(y.item())\n",
    "        del mask\n",
    "        del datapoint\n",
    "        del one_new_timestep\n",
    "        gc.collect(),torch.cuda.empty_cache()\n",
    "        \n",
    "        if (n%100==0):\n",
    "            print('{}/{}'.format(n,n_samples))\n",
    "        if ((n+1)%1000==0):\n",
    "             np.savez('npz_transformer_token_V11.1',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n",
    "\n",
    "max_length = 400\n",
    "n_seed = 2\n",
    "# Padding Mask Fed here is the Mask where \"False is Real Data\", True is masked and ignore them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a648f5c-83e8-4843-8530-e257e5144aff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "0/8062\n",
      "Actual Length is tensor(10)\n",
      "9 tensor(10)\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/yelnady/.local/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(34)\n",
      "9 tensor(34)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "3 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "9 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(4)\n",
      "9 tensor(4)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(10)\n",
      "8 tensor(10)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "3 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(10)\n",
      "9 tensor(10)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(5)\n",
      "9 tensor(5)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(4)\n",
      "4 tensor(4)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(10)\n",
      "8 tensor(10)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "32 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "3 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "9 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "2 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(16)\n",
      "5 tensor(16)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "4 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "3 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(4)\n",
      "5 tensor(4)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "9 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(19)\n",
      "9 tensor(19)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "100/8062\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "3 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(4)\n",
      "3 tensor(4)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "2 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(10)\n",
      "9 tensor(10)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(11)\n",
      "3 tensor(11)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(10)\n",
      "8 tensor(10)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(5)\n",
      "9 tensor(5)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(11)\n",
      "44 tensor(11)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(4)\n",
      "3 tensor(4)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(10)\n",
      "26 tensor(10)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "5 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(3)\n",
      "3 tensor(3)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(7)\n",
      "3 tensor(7)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(9)\n",
      "8 tensor(9)\n",
      "Done\n",
      "Actual Length is tensor(2)\n",
      "2 tensor(2)\n",
      "Done\n",
      "Actual Length is tensor(2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-01287df63ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# generate_dataset(real_train_X0 ,real_train_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_train_X0.size(0),max_length=max_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_train_X3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreal_train_Y_labels3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpadding_mask3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_train_X3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-2365b1dee495>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[0;34m(X, Y, masks, n_seed, n_samples, max_length)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdatapoint_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdatapoint_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mnlp_predicted\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoint_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Convert to probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Choose from the top k , returns values (probs) and indices (labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0ed4592bdfc5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, padding_mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \"\"\"\n\u001b[1;32m    320\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0;32m--> 321\u001b[0;31m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4967\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4969\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4970\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4971\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4733\u001b[0m             \u001b[0mb_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4734\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "real_train_X0 ,real_train_Y_labels0 ,padding_mask0= get_one_class(real_train_X ,real_train_Y_labels ,real_train_masks,0)\n",
    "real_train_X1 ,real_train_Y_labels1 ,padding_mask1= get_one_class(real_train_X ,real_train_Y_labels ,real_train_masks,1)\n",
    "real_train_X2 ,real_train_Y_labels2 ,padding_mask2= get_one_class(real_train_X ,real_train_Y_labels ,real_train_masks,2)\n",
    "real_train_X3 ,real_train_Y_labels3 ,padding_mask3= get_one_class(real_train_X ,real_train_Y_labels ,real_train_masks,3)\n",
    "\n",
    "\n",
    "generate_dataset(real_train_X0 ,real_train_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_train_X0.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X1 ,real_train_Y_labels1 ,padding_mask1,n_seed=n_seed,n_samples=real_train_X1.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X2 ,real_train_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_train_X2.size(0),max_length=max_length)\n",
    "generate_dataset(real_train_X3 ,real_train_Y_labels3 ,padding_mask3,n_seed=n_seed,n_samples=real_train_X3.size(0),max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c81528-2a26-43a9-92c7-a13b65684ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to make sure everything is calculated correctly in softmax and argmax\n",
    "\n",
    "# x = torch.rand((2,3))\n",
    "# print(x)\n",
    "# print(F.softmax(x).argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ab8ea-f7f2-4c77-84e8-965c3a281831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# real_test_X0 ,real_test_Y_labels0 ,padding_mask0= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,0)\n",
    "# real_test_X1 ,real_test_Y_labels1 ,padding_mask1= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,1)\n",
    "# real_test_X2 ,real_test_Y_labels2 ,padding_mask2= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,2)\n",
    "# real_test_X3 ,real_test_Y_labels3 ,padding_mask3= get_one_class(real_test_X ,real_test_Y_labels ,real_test_masks,3)\n",
    "\n",
    "\n",
    "# generate_dataset(real_test_X0 ,real_test_Y_labels0 ,padding_mask0,n_seed=n_seed,n_samples=real_test_X0.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X1 ,real_test_Y_labels1 ,padding_mask1,n_seed=n_seed,n_samples=real_test_X1.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X2 ,real_test_Y_labels2 ,padding_mask2,n_seed=n_seed,n_samples=real_test_X2.size(0),max_length=max_length)\n",
    "# generate_dataset(real_test_X3 ,real_test_Y_labels3 ,padding_mask3,n_seed=n_seed,n_samples=real_test_X3.size(0),max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193017c-1930-4e3e-bdaa-29a9fcb36697",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('npz_transformer_token_V11',X=generated_dataset_X,masks= resulted_masks,Y=generated_dataset_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474e225-9e29-41a4-a74c-36e2d292d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c638b2-11d3-4bd1-a128-1a225e530077",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect(),torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d6dec-bdbb-4961-956c-28eb2390b470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What is the average values of all samples in variable length sequence\n",
    "\n",
    "mean_of_samples = []\n",
    "for x,length in zip(real_train_X, real_train_lengths):\n",
    "    mean_of_samples.append(torch.sum(x)/(9*length)) # torch.Size([2500])\n",
    "    \n",
    "print(np.mean(mean_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e2e56-6e32-4d9c-b04f-d0eb8356742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max(input)  Tensor\n",
    "# Returns the maximum value of all elements in the input tensor.\n",
    "\n",
    "torch.max(real_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71072b7-4b93-41d4-9894-c81bd6686485",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(real_train_X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0674c9-e124-4dd3-99be-e7dc8ca16a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(real_train_X.flatten(), 'o', color='black');\n",
    "plt.xscale(\"log\")\n",
    "ax.set_title('CDF - Class all')\n",
    "ax.set_xlabel('The Sequence Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8cf9d-b707-430d-bf46-1f13e24d8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(real_train_X,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134539e-d658-4fa6-87f0-f75d99814867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(torch.mean(real_train_X,2).flatten(), 'o', color='black');\n",
    "plt.xscale(\"log\")\n",
    "ax.set_title('CDF - Class all')\n",
    "ax.set_xlabel('The Sequence Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6aa925-a487-46f4-b536-4cb11877f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([1,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d43f4-af58-47d6-abdb-bdb7c4e3c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
