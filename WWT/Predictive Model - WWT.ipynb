{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stylish-logistics",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retired-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "sys.path.append('../DG/gan')\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score\n",
    "# pl.utilities.seed.seed_everything(seed=2) # sets seed for pseudo-random number generators in: pytorch, numpy, python.random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-winter",
   "metadata": {},
   "source": [
    "# Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "undefined-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each has three arrays: data_feature,data_attribute,data_gen_flag \n",
    "training_real = np.load('../data/web/data_train.npz')\n",
    "test_real = np.load('../data/web/data_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regional-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################REAL#################################################\n",
    "# Y here means the data that we are going to forecast it\n",
    "real_train_X =  torch.from_numpy(training_real['data_feature'][:,:500]).float()\n",
    "real_train_Y =  torch.from_numpy(training_real['data_feature'][:,500:]).float()\n",
    "\n",
    "real_test_X =  torch.from_numpy(test_real['data_feature'][:,:500]).float()\n",
    "real_test_Y =  torch.from_numpy(test_real['data_feature'][:,500:]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-mineral",
   "metadata": {},
   "source": [
    "# Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "black-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_DG = np.load('../data_generated/web/generated_data_train.npz')\n",
    "TST = np.load('WWT_generated_new.npz')\n",
    "#########################################DG#################################################\n",
    "DG_X =  torch.from_numpy(training_DG['data_feature'][:,:500]).float()\n",
    "DG_Y =  torch.from_numpy(training_DG['data_feature'][:,500:]).float()\n",
    "\n",
    "#########################################TST#################################################\n",
    "TST_X =  torch.from_numpy(TST['X'][:,:500]).float()\n",
    "TST_Y =  torch.from_numpy(TST['X'][:,500:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "meaningful-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49999, 500, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TST_X[:-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-premiere",
   "metadata": {},
   "source": [
    "# Features & Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aware-lotus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Features\n",
      "Feature: 1  -- Normalization: Normalization.MINUSONE_ONE  -- gen_flag: False  -- Dim: 1\n",
      "\n",
      "Y Features\n",
      "Feature: 1  -- Normalization: None  -- gen_flag: False  -- Dim: 9\n",
      "Feature: 2  -- Normalization: None  -- gen_flag: False  -- Dim: 3\n",
      "Feature: 3  -- Normalization: None  -- gen_flag: False  -- Dim: 2\n"
     ]
    }
   ],
   "source": [
    "with open('../data/web/data_feature_output.pkl', 'rb') as f:\n",
    "    data_feature = pickle.load(f)    \n",
    "with open('../data/web/data_attribute_output.pkl', 'rb') as f:\n",
    "    data_attribute = pickle.load(f)\n",
    "\n",
    "    \n",
    "# data_feature is a list of 9 \"output.Output\" objects, where each object contains attrs -> (is_gen_flag, dim, normalization)\n",
    "print(\"X Features\")\n",
    "for i,feature in enumerate(data_feature):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)\n",
    "\n",
    "print(\"\\nY Features\")\n",
    "for i,feature in enumerate(data_attribute):\n",
    "    print(\"Feature:\",i+1,\" -- Normalization:\",feature.normalization, \" -- gen_flag:\",feature.is_gen_flag, \" -- Dim:\",feature.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-velvet",
   "metadata": {},
   "source": [
    "# Regressor Model - 1-MlP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sacred-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel1(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                  nn.Flatten(start_dim=1),\n",
    "                  nn.Linear(500,100),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(100,50),\n",
    "                )\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y.squeeze()) \n",
    "        r2score = r2_score(y_hat.cpu().detach().numpy() , y.squeeze().cpu() )\n",
    "        self.log('r2_score',r2score, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss,'r2_score':r2score}\n",
    "    \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y.squeeze(2))\n",
    "        r2score = r2_score(y_hat.cpu().detach().numpy() , y.squeeze(2).cpu() )\n",
    "        self.log('r2_score',r2score, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss,'r2_score':r2score}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "german-polymer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 55.1 K\n",
      "-------------------------------------\n",
      "55.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "55.1 K    Total params\n",
      "0.221     Total estimated model params size (MB)\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef46c2394e941a8812adde8f5cc8e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca02da66a91d480a837b15206f78ccea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'r2_score': 0.8322919607162476}\n",
      "--------------------------------------------------------------------------------\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "params = {'shuffle': True,'num_workers': 0,'batch_size':256}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.FloatTensor(TST_X[:]).to(device),torch.FloatTensor(TST_Y[:]).to(device))\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.FloatTensor(real_train_X).to(device),torch.FloatTensor(real_train_Y).to(device))\n",
    "    dataset = torch.utils.data.TensorDataset(torch.FloatTensor(DG_X).to(device),torch.FloatTensor(DG_Y).to(device))\n",
    "\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.cat((TST_all_X[:,:400].to(device),real_train_X[:percent,:400].to(device))),\n",
    "#                                              torch.cat((TST_all_Y_labels.long().to(device),real_train_Y_labels[:percent].to(device))))\n",
    "\n",
    "    train_dataloader  = torch.utils.data.DataLoader(dataset, **params)\n",
    "    \n",
    "    dataset =  torch.utils.data.TensorDataset(torch.FloatTensor(real_test_X).to(device),torch.FloatTensor(real_test_Y).to(device))\n",
    "    test_dataloader  = torch.utils.data.DataLoader(dataset,batch_size=256)\n",
    "    model = MLPModel1()\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=100,progress_bar_refresh_rate=1)\n",
    "trainer.fit(model,train_dataloader)\n",
    "trainer.test(test_dataloaders=test_dataloader) # No need to repass (model), It will by itself work from test_step\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "appreciated-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 Score - 1-MLP Layer\n",
    "# Real Data: 0.8915\n",
    "# DG: 0.785\n",
    "# TST: 0.711\n",
    "\n",
    "# TST: 0.7754 using only 5000 samples\n",
    "# TST: 0.7547 using only 28,000 samples\n",
    "# TST: 0.716 using only 34,000 samples\n",
    "\n",
    "# In TST, using 5 epochs is more than enough to get an r2 score of 0.787\n",
    "# In TST, using 5 epochs and 43,000 samples -> 0.779\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-sauce",
   "metadata": {},
   "source": [
    "# MLP 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "killing-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel5(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                  nn.Flatten(start_dim=1),\n",
    "                  nn.Linear(500,200),nn.ReLU(),\n",
    "                  nn.Linear(200,200),nn.ReLU(),\n",
    "                  nn.Linear(200,200),nn.ReLU(),\n",
    "                  nn.Linear(200,200),nn.ReLU(),\n",
    "                  nn.Linear(200,200),nn.ReLU(),\n",
    "                  nn.Linear(200,50),\n",
    "                )\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y.squeeze()) \n",
    "        r2score = r2_score(y_hat.cpu().detach().numpy() , y.squeeze().cpu() )\n",
    "        self.log('r2_score',r2score, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss,'r2_score':r2score}\n",
    "    \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y.squeeze(2))\n",
    "        r2score = r2_score(y_hat.cpu().detach().numpy() , y.squeeze(2).cpu() )\n",
    "        self.log('r2_score',r2score, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {'loss': loss,'r2_score':r2score}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nutritional-canberra",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 110 K \n",
      "-------------------------------------\n",
      "110 K     Trainable params\n",
      "0         Non-trainable params\n",
      "110 K     Total params\n",
      "0.441     Total estimated model params size (MB)\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7629dd109f004001baea3d025e64bc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/rhome/yelnady/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a8b56cb4d74ea89f384119496a1555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'r2_score': 0.7324246168136597}\n",
      "--------------------------------------------------------------------------------\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "params = {'shuffle': True,'num_workers': 0,'batch_size':64}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = torch.utils.data.TensorDataset(torch.FloatTensor(TST_X).to(device),torch.FloatTensor(TST_Y).to(device))\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.FloatTensor(real_train_X).to(device),torch.FloatTensor(real_train_Y).to(device))\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.FloatTensor(DG_X).to(device),torch.FloatTensor(DG_Y).to(device))\n",
    "\n",
    "#     dataset = torch.utils.data.TensorDataset(torch.cat((TST_all_X[:,:400].to(device),real_train_X[:percent,:400].to(device))),\n",
    "#                                              torch.cat((TST_all_Y_labels.long().to(device),real_train_Y_labels[:percent].to(device))))\n",
    "\n",
    "    train_dataloader  = torch.utils.data.DataLoader(dataset, **params)\n",
    "    \n",
    "    dataset =  torch.utils.data.TensorDataset(torch.FloatTensor(real_test_X).to(device),torch.FloatTensor(real_test_Y).to(device))\n",
    "    test_dataloader  = torch.utils.data.DataLoader(dataset,batch_size=16)\n",
    "    model = MLPModel1()\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=100,progress_bar_refresh_rate=1)\n",
    "trainer.fit(model,train_dataloader)\n",
    "trainer.test(test_dataloaders=test_dataloader) # No need to repass (model), It will by itself work from test_step\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-calculator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sharp-relaxation",
   "metadata": {},
   "source": [
    "# Other ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "historical-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_models(train_X, train_Y,test_X,test_Y):\n",
    "    lor = LinearRegression()\n",
    "    lor.fit(train_X, train_Y)\n",
    "    print('Linear Regression',lor.score(test_X,test_Y))\n",
    "    \n",
    "    kr = KernelRidge()\n",
    "    kr.fit(train_X,train_Y)\n",
    "    print('Kernel Ridge',kr.score(test_X,test_Y)) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "boring-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression -467.3841332060692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/yelnady/.local/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=2.5007e-09): result may not be accurate.\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge 0.8117641893542101\n"
     ]
    }
   ],
   "source": [
    "other_models(torch.flatten(TST_X,1),torch.flatten(TST_Y,1),torch.flatten(real_test_X,1),torch.flatten(real_test_Y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fresh-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression 0.9088504188664293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/yelnady/.local/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=1.73174e-09): result may not be accurate.\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge 0.90875846680532\n"
     ]
    }
   ],
   "source": [
    "other_models(torch.flatten(real_train_X,1),torch.flatten(real_train_Y,1),torch.flatten(real_test_X,1),torch.flatten(real_test_Y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "robust-semester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression 0.8132905114602174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/yelnady/.local/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=1.07368e-09): result may not be accurate.\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge 0.8206123184299539\n"
     ]
    }
   ],
   "source": [
    "other_models(torch.flatten(DG_X,1),torch.flatten(DG_Y,1),torch.flatten(real_test_X,1),torch.flatten(real_test_Y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opened-region",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression 0.9007136549766838\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 37.3 GiB for an array with shape (100000, 100000) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-722cebe19aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m other_models(torch.flatten(torch.cat((real_train_X,DG_X)),1),torch.flatten(torch.cat((real_train_Y,DG_Y)),1),\n\u001b[0;32m----> 2\u001b[0;31m              torch.flatten(real_test_X,1),torch.flatten(real_test_Y,1))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-bdb68fffb486>\u001b[0m in \u001b[0;36mother_models\u001b[0;34m(train_X, train_Y, test_X, test_Y)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kernel Ridge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/kernel_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/kernel_ridge.py\u001b[0m in \u001b[0;36m_get_kernel\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    132\u001b[0m                       \"coef0\": self.coef0}\n\u001b[1;32m    133\u001b[0m         return pairwise_kernels(X, Y, metric=self.kernel,\n\u001b[0;32m--> 134\u001b[0;31m                                 filter_params=True, **params)\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_kernels\u001b[0;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown kernel %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1954\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mlinear_kernel\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \"\"\"\n\u001b[1;32m   1004\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 37.3 GiB for an array with shape (100000, 100000) and data type float32"
     ]
    }
   ],
   "source": [
    "other_models(torch.flatten(torch.cat((real_train_X,TST_X)),1),torch.flatten(torch.cat((real_train_Y,TST_Y)),1),\n",
    "             torch.flatten(real_test_X,1),torch.flatten(real_test_Y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-stevens",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
